{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T22:38:21.954685Z",
     "iopub.status.busy": "2022-12-01T22:38:21.954114Z",
     "iopub.status.idle": "2022-12-01T22:38:34.294692Z",
     "shell.execute_reply": "2022-12-01T22:38:34.294127Z",
     "shell.execute_reply.started": "2022-12-01T22:38:21.954623Z"
    },
    "id": "Ja6ImtacAEEK",
    "outputId": "91376c1a-daf0-4074-ddb0-41234bae440b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.5.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.0.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.64.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.4.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2022.5.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.23.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.8.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.13)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.3.2)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (8.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.14.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (18.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from rouge-score) (1.1.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from rouge-score) (3.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rouge-score) (1.23.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge-score) (1.14.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (2022.7.9)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (4.64.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (8.1.3)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=777c915bac202afa124705a490001a928de7fe13da392c80e38e9f8e015faf3e\n",
      "  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.20.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.28.1)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.1.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb) (1.14.0)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Building wheels for collected packages: promise, pathtools\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=8eda5679c3db9de922bad960ae99b0aee6843326ad426ae7c07bda4203bcaa03\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/e8/83/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=e5558b082abf11ecc9593c61aad0ca586ab57916455c4a669933490986dbc2f3\n",
      "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built promise pathtools\n",
      "Installing collected packages: pathtools, urllib3, shortuuid, setproctitle, promise, docker-pycreds, sentry-sdk, wandb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.10\n",
      "    Uninstalling urllib3-1.26.10:\n",
      "      Successfully uninstalled urllib3-1.26.10\n",
      "Successfully installed docker-pycreds-0.4.0 pathtools-0.1.2 promise-2.3 sentry-sdk-1.11.1 setproctitle-1.3.2 shortuuid-1.0.11 urllib3-1.26.13 wandb-0.13.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install evaluate\n",
    "%pip install rouge-score\n",
    "%pip install transformers\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eBEmrRCLx1iY",
    "outputId": "fe407f29-7236-4518-dd28-307e4c2cb035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i60EEs-QzNoY",
    "outputId": "fa349c34-9177-407f-a9ca-56d3f26ac897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/diff-lm\n",
      "\u001b[0m\u001b[01;34mbase\u001b[0m/  \u001b[01;34mlogs\u001b[0m/                 \u001b[01;34mmodelsbo\u001b[0m/    \u001b[01;34mmodelstabd-1\u001b[0m/  \u001b[01;34mwandb\u001b[0m/\n",
      "\u001b[01;34mdata\u001b[0m/  \u001b[01;34mmodels-base-uncased\u001b[0m/  \u001b[01;34mmodelstabd\u001b[0m/  samples.txt\n"
     ]
    }
   ],
   "source": [
    "%cd drive/MyDrive/diff-lm/\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T22:38:41.995671Z",
     "iopub.status.busy": "2022-12-01T22:38:41.995397Z",
     "iopub.status.idle": "2022-12-01T22:38:41.999043Z",
     "shell.execute_reply": "2022-12-01T22:38:41.998535Z",
     "shell.execute_reply.started": "2022-12-01T22:38:41.995648Z"
    },
    "id": "B2HA6Vil3Ihb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# os.listdir('data')\n",
    "# data = []\n",
    "# with open(\"data/calendar.dev.jsonl\") as f:\n",
    "#     for line in f:\n",
    "#         a=json.loads(line)\n",
    "#         a[\"formula\"] = a[\"formula\"].replace(\"edu.stanford.nlp.sempre.overnight.SimpleWorld.\", \"\")\n",
    "#         data.append(a)\n",
    "\n",
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T22:38:45.853455Z",
     "iopub.status.busy": "2022-12-01T22:38:45.853177Z",
     "iopub.status.idle": "2022-12-01T22:38:46.176769Z",
     "shell.execute_reply": "2022-12-01T22:38:46.176297Z",
     "shell.execute_reply.started": "2022-12-01T22:38:45.853432Z"
    },
    "id": "UEG_5Kfm4gmf",
    "outputId": "c5d0a8ec-d735-4655-b535-192b6b4794c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'canonical': 'meeting whose end time is larger than 10am or 3pm',\n",
       " 'formula': '(call listValue (call filter (call getProperty (call singleton en.meeting) (string !type)) (call ensureNumericProperty (string end_time)) (string >) (call ensureNumericEntity (call concat (time 10 0) (time 15 0)))))',\n",
       " 'natural': 'which meetings end later than 10 in the morning or 3 in the afternoon'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOMAINS = (\n",
    "    \"calendar\",\n",
    "    \"basketball\",\n",
    "    \"blocks\",\n",
    "    \"housing\",\n",
    "    \"publications\",\n",
    "    \"recipes\",\n",
    "    \"restaurants\",\n",
    "    \"socialnetwork\",\n",
    ")\n",
    "\n",
    "def get_data(domain, dataset=\"train_with_dev\"):\n",
    "    data = []\n",
    "    with open(\"data/\" + domain + \".\" + dataset + \".jsonl\") as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line)\n",
    "            record[\"formula\"] = simplifier(record[\"formula\"])\n",
    "            data.append(record)\n",
    "    return data\n",
    "\n",
    "simplifier = lambda txt: txt.replace(\"edu.stanford.nlp.sempre.overnight.SimpleWorld.\", \"\")\n",
    "train_all = {}\n",
    "test_all = {}\n",
    "for domain in DOMAINS:\n",
    "    train_all[domain] = get_data(domain)\n",
    "    test_all[domain] = get_data(domain, dataset=\"test\")\n",
    "\n",
    "# train_all[DOMAINS[0]][0]\n",
    "# dev_all[DOMAINS[0]][0]\n",
    "test_all[DOMAINS[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T22:38:55.963273Z",
     "iopub.status.busy": "2022-12-01T22:38:55.962548Z",
     "iopub.status.idle": "2022-12-01T22:38:55.971491Z",
     "shell.execute_reply": "2022-12-01T22:38:55.970841Z",
     "shell.execute_reply.started": "2022-12-01T22:38:55.963239Z"
    },
    "id": "8-Dv08Cf8XbJ",
    "outputId": "e3c9b72d-68e1-49a7-bbaf-956353482ed9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_train_dev(domain, domains_data, train_size=200, remain_dev=0.2, shuffle=True):\n",
    "  data = domains_data[domain]\n",
    "  if shuffle:\n",
    "    np.random.shuffle(data)\n",
    "  size = len(data)\n",
    "  dev_size = np.ceil((size - train_size) * 0.2).astype(int) + train_size\n",
    "  return data[:train_size], data[train_size:dev_size]\n",
    "\n",
    "train_dict = {}\n",
    "dev_dict = {}\n",
    "for domain in DOMAINS:\n",
    "  train_dict[domain], dev_dict[domain] = split_train_dev(domain, train_all)\n",
    "\n",
    "len(train_dict[DOMAINS[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T22:38:58.658359Z",
     "iopub.status.busy": "2022-12-01T22:38:58.657484Z",
     "iopub.status.idle": "2022-12-01T22:38:58.662151Z",
     "shell.execute_reply": "2022-12-01T22:38:58.661669Z",
     "shell.execute_reply.started": "2022-12-01T22:38:58.658328Z"
    },
    "id": "6kqoTd7OgBlq"
   },
   "outputs": [],
   "source": [
    "def prepare_data(data, shuffle=True):\n",
    "  inputs = []\n",
    "  outputs = []\n",
    "  for domain in DOMAINS:\n",
    "    domain_data = data[domain]\n",
    "    if shuffle:\n",
    "      np.random.shuffle(domain_data)\n",
    "    for record in domain_data:\n",
    "      inputs.append(record['natural'])\n",
    "      outputs.append(record['canonical'])\n",
    "  return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T22:39:01.228617Z",
     "iopub.status.busy": "2022-12-01T22:39:01.228358Z",
     "iopub.status.idle": "2022-12-01T22:39:04.054484Z",
     "shell.execute_reply": "2022-12-01T22:39:04.053866Z",
     "shell.execute_reply.started": "2022-12-01T22:39:01.228598Z"
    },
    "id": "Q1Cn4rmp3D9G"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers import AutoModelForPreTraining,AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T22:39:19.057853Z",
     "iopub.status.busy": "2022-12-01T22:39:19.057318Z",
     "iopub.status.idle": "2022-12-01T22:39:21.625304Z",
     "shell.execute_reply": "2022-12-01T22:39:21.624690Z",
     "shell.execute_reply.started": "2022-12-01T22:39:19.057830Z"
    },
    "id": "F0dckuHUyW9W",
    "outputId": "4c2ec806-d9b2-4cec-b130-5945083e218f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T22:39:17.045966Z",
     "iopub.status.busy": "2022-12-01T22:39:17.044902Z",
     "iopub.status.idle": "2022-12-01T22:39:17.049387Z",
     "shell.execute_reply": "2022-12-01T22:39:17.048915Z",
     "shell.execute_reply.started": "2022-12-01T22:39:17.045939Z"
    },
    "id": "pFglgJLUyZG9",
    "outputId": "35f70522-6f33-40d5-a730-5b2eb071a4e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=diff_lm_semantic_parsing\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=diff_lm_semantic_parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T22:39:31.688558Z",
     "iopub.status.busy": "2022-12-01T22:39:31.687827Z",
     "iopub.status.idle": "2022-12-01T22:39:31.697221Z",
     "shell.execute_reply": "2022-12-01T22:39:31.696714Z",
     "shell.execute_reply.started": "2022-12-01T22:39:31.688533Z"
    },
    "id": "KX-v3wbi3EuP"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to = 'wandb',  \n",
    "    run_name=\"true-diff-final-170\",\n",
    "    output_dir='./true-diff-final-170',          # output directory\n",
    "    num_train_epochs=20,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_accumulation_steps=10,\n",
    "    label_names=['labels'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T22:39:34.541247Z",
     "iopub.status.busy": "2022-12-01T22:39:34.540625Z",
     "iopub.status.idle": "2022-12-01T22:39:34.546647Z",
     "shell.execute_reply": "2022-12-01T22:39:34.546154Z",
     "shell.execute_reply.started": "2022-12-01T22:39:34.541224Z"
    },
    "id": "_T0QrAntYAaT"
   },
   "outputs": [],
   "source": [
    "class OvernightDataset(Dataset): \n",
    "    def __init__(self, data, init_model, max_len, func=prepare_data):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(init_model)\n",
    "        self.inputs, self.labels = func(data)\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer.model_max_length = max_len\n",
    "    def __getitem__(self, index):\n",
    "        from_tokenizer = self.tokenizer(self.inputs[index],padding=\"max_length\",truncation = True,return_tensors=\"pt\")\n",
    "        label_tokens = self.tokenizer(self.labels[index],padding=\"max_length\",truncation = True,return_tensors=\"pt\")\n",
    "        input_ids = from_tokenizer[\"input_ids\"].squeeze_().long()\n",
    "        ret_labels = label_tokens[\"input_ids\"].squeeze_().long()\n",
    "        token_type_ids = from_tokenizer[\"token_type_ids\"].squeeze_().long()\n",
    "        attention_mask = from_tokenizer[\"attention_mask\"].squeeze_().long()\n",
    "        labels_token_type_ids = label_tokens[\"token_type_ids\"].squeeze_().long()\n",
    "        labels_attention_mask = label_tokens[\"attention_mask\"].squeeze_().long()\n",
    "        # return input_ids,token_type_ids,attention_mask\n",
    "        return {\"input_ids\": input_ids, \n",
    "                \"token_type_ids\" : token_type_ids, \n",
    "                \"attention_mask\" : attention_mask, \n",
    "                \"labels\" : ret_labels, \n",
    "                \"labels_token_type_ids\" : labels_token_type_ids, \n",
    "                \"labels_attention_mask\" : labels_attention_mask}\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T22:39:37.286389Z",
     "iopub.status.busy": "2022-12-01T22:39:37.285823Z",
     "iopub.status.idle": "2022-12-01T22:39:37.301245Z",
     "shell.execute_reply": "2022-12-01T22:39:37.300736Z",
     "shell.execute_reply.started": "2022-12-01T22:39:37.286367Z"
    },
    "id": "OcUyys55lVWo"
   },
   "outputs": [],
   "source": [
    "class diffusion_bert(nn.Module):\n",
    "    def __init__(self,init_model,max_len,max_step,k=1) -> None:\n",
    "        super().__init__()\n",
    "        if \"bert-base\" in init_model:\n",
    "            self.model = AutoModelForMaskedLM.from_pretrained(init_model)\n",
    "            freezed_w = [self.model.bert.embeddings.token_type_embeddings.weight,self.model.bert.embeddings.word_embeddings.weight] #self.model.bert.embeddings.LayerNorm.weight, self.model.bert.embeddings.LayerNorm.bias\n",
    "        else:\n",
    "            self.model = AutoModelForPreTraining.from_pretrained(init_model)\n",
    "            freezed_w = [self.model.cls.seq_relationship.bias, self.model.cls.seq_relationship.weight, self.model.bert.pooler.dense.bias, self.model.bert.pooler.dense.weight, self.model.bert.embeddings.token_type_embeddings.weight,self.model.bert.embeddings.word_embeddings.weight] #self.model.bert.embeddings.LayerNorm.weight, self.model.bert.embeddings.LayerNorm.bias\n",
    "        self.max_len = max_len\n",
    "        self.max_step = max_step\n",
    "        self.k=k\n",
    "        self.time_embed = nn.Embedding(max_step,self.model.config.hidden_size)\n",
    "        #self.layernorm = nn.LayerNorm(self.model.config.hidden_size, eps=self.model.config.layer_norm_eps)\n",
    "        for p in  freezed_w:\n",
    "            p.requires_grad = False\n",
    "        nn.init.constant_(self.time_embed.weight, 0)\n",
    "    def forward(self,input_ids,token_type_ids,attention_mask, labels, labels_token_type_ids, labels_attention_mask):\n",
    "        t = self.max_step\n",
    "        input_shape = input_ids.size()\n",
    "        seq_length = input_shape[1]\n",
    "        \n",
    "        position_ids = self.model.bert.embeddings.position_ids[:, 0 : seq_length]\n",
    "        position_embeddings = self.model.bert.embeddings.position_embeddings(position_ids)\n",
    "\n",
    "\n",
    "        # Trial 31:\n",
    "        output_shape = labels.size()\n",
    "        out_seq_length = output_shape[1]\n",
    "        N = input_shape[0]\n",
    "        # outpos_ids = self.model.bert.embeddings.position_ids[:, 0 : out_seq_length]\n",
    "        # out_pos_embeddings = self.model.bert.embeddings.position_embeddings(outpos_ids)\n",
    "\n",
    "       \n",
    "        with torch.no_grad():\n",
    "            target_emb = self.model.bert.embeddings.word_embeddings(labels)\n",
    "            inp_emb = self.model.bert.embeddings.word_embeddings(input_ids)\n",
    "        #print(word_emb.shape)\n",
    "            token_type_embeddings = self.model.bert.embeddings.token_type_embeddings(token_type_ids)\n",
    "        # labels_token_type_embeddings = self.model.bert.embeddings.token_type_embeddings(labels_token_type_ids)\n",
    "        \n",
    "        \n",
    "            xt = torch.normal(0,1,(N,self.max_len,self.model.config.hidden_size)).to(device) #/ math.sqrt(self.model.config.hidden_size)\n",
    "            xt_token_type_ids = torch.zeros(N,self.max_len).long().to(device)\n",
    "            attention_mask = torch.ones(N,self.max_len).long().to(device)\n",
    "            extended_attention_mask = self.model.bert.get_extended_attention_mask(attention_mask, attention_mask.shape)\n",
    "            xt_token_type_embeddings = self.model.bert.embeddings.token_type_embeddings(token_type_ids)\n",
    "        xt_position_ids = self.model.bert.embeddings.position_ids[:, 0 : self.max_len]\n",
    "        xt_position_embeddings = self.model.bert.embeddings.position_embeddings(position_ids)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss_x0 = None\n",
    "        for t in range(self.max_step-1,0,-1):\n",
    "            # print(\"Step\", t)\n",
    "            diffusion_steps = torch.ones(size = (output_shape[0],),device=input_ids.device).long()*t\n",
    "            time_embedding = self.time_embed(diffusion_steps).unsqueeze(1)\n",
    "\n",
    "            model_input = inp_emb+xt+position_embeddings+xt_position_embeddings+time_embedding\n",
    "            model_input = self.model.bert.embeddings.LayerNorm(model_input)\n",
    "            #denoise\n",
    "            encoder_outputs = self.model.bert.encoder(\n",
    "                model_input,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                head_mask=[None] * self.model.config.num_hidden_layers\n",
    "            )\n",
    "            sequence_output = encoder_outputs[0]\n",
    "            prediction_scores = self.model.cls.predictions(sequence_output)\n",
    "\n",
    "            #clamp\n",
    "            # pred = torch.argmax(prediction_scores,-1).long()\n",
    "            # denoised_word = self.model.bert.embeddings.word_embeddings(pred)\n",
    "            denoised_word = prediction_scores.softmax(-1) @ self.model.bert.embeddings.word_embeddings.weight.unsqueeze(0)\n",
    "\n",
    "            if loss_x0 == None:\n",
    "            # loss_x0 = F.cross_entropy(prediction_scores.view(-1, self.model.config.vocab_size),labels.flatten(),ignore_index=0)\n",
    "                loss_x0 = F.mse_loss(denoised_word, target_emb)\n",
    "            else:\n",
    "            #     loss_x0 += F.cross_entropy(prediction_scores.view(-1, self.model.config.vocab_size),labels.flatten(),ignore_index=0)\n",
    "                loss_x0 += F.mse_loss(denoised_word, target_emb)\n",
    "            #DDIM\n",
    "            alpha_tk = 1 - math.sqrt((t)/self.max_step)#+1e-5\n",
    "            alpha_t = 1 - math.sqrt((t+1)/self.max_step)+1e-5\n",
    "            noise = (xt - math.sqrt(alpha_t)*denoised_word)/math.sqrt(1-alpha_t)\n",
    "            xt = math.sqrt(alpha_tk)*(xt/math.sqrt(alpha_t) + (math.sqrt((1-alpha_tk)/alpha_tk) - math.sqrt((1-alpha_t)/alpha_t))*noise)\n",
    "            #noisy_word = math.sqrt(alpha_tk)*denoised_word + math.sqrt(1-alpha_tk)*noise\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        prediction_scores = self.model.cls.predictions(xt)\n",
    "        loss_emb = F.mse_loss(xt, target_emb)\n",
    "        loss_round = F.cross_entropy(prediction_scores.view(-1, self.model.config.vocab_size),labels.flatten(),ignore_index=0)\n",
    "        \n",
    "        loss = loss_x0 + loss_emb + loss_round\n",
    "        #loss = F.smooth_l1_loss(sequence_output,word_emb)\n",
    "        return loss,prediction_scores,labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     def foval(self,input_ids,token_type_ids,attention_mask, labels, labels_token_type_ids, labels_attention_mask):\n",
    "#         t = self.max_step\n",
    "#         input_shape = input_ids.size()\n",
    "#         seq_length = input_shape[1]\n",
    "        \n",
    "#         position_ids = self.model.bert.embeddings.position_ids[:, 0 : seq_length]\n",
    "#         position_embeddings = self.model.bert.embeddings.position_embeddings(position_ids)\n",
    "\n",
    "\n",
    "#         # Trial 31:\n",
    "#         output_shape = labels.size()\n",
    "#         out_seq_length = output_shape[1]\n",
    "#         N = input_shape[0]\n",
    "#         # outpos_ids = self.model.bert.embeddings.position_ids[:, 0 : out_seq_length]\n",
    "#         # out_pos_embeddings = self.model.bert.embeddings.position_embeddings(outpos_ids)\n",
    "\n",
    "       \n",
    "#         # with torch.no_grad():\n",
    "#         target_emb = self.model.bert.embeddings.word_embeddings(labels)\n",
    "#         inp_emb = self.model.bert.embeddings.word_embeddings(input_ids)\n",
    "#         #print(word_emb.shape)\n",
    "#         token_type_embeddings = self.model.bert.embeddings.token_type_embeddings(token_type_ids)\n",
    "#         # labels_token_type_embeddings = self.model.bert.embeddings.token_type_embeddings(labels_token_type_ids)\n",
    "        \n",
    "        \n",
    "#         xt = torch.normal(0,1,(N,self.max_len,self.model.config.hidden_size)).to(device) #/ math.sqrt(self.model.config.hidden_size)\n",
    "#         xt_token_type_ids = torch.zeros(N,self.max_len).long().to(device)\n",
    "#         attention_mask = torch.ones(N,self.max_len).long().to(device)\n",
    "#         extended_attention_mask = self.model.bert.get_extended_attention_mask(attention_mask, attention_mask.shape)\n",
    "\n",
    "#         xt_position_ids = self.model.bert.embeddings.position_ids[:, 0 : self.max_len]\n",
    "#         xt_position_embeddings = self.model.bert.embeddings.position_embeddings(position_ids)\n",
    "#         xt_token_type_embeddings = self.model.bert.embeddings.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        \n",
    "#         # loss_x0 = None\n",
    "#         for t in range(self.max_step-1,0,-1):\n",
    "#             # print(\"Step\", t)\n",
    "#             # exit()\n",
    "#             diffusion_steps = torch.ones(size = (N,),device=device).long()*t\n",
    "#             time_embedding = self.time_embed(diffusion_steps).unsqueeze(1)\n",
    "\n",
    "#             model_input = inp_emb+xt+position_embeddings+xt_position_embeddings+time_embedding\n",
    "#             model_input = self.model.bert.embeddings.LayerNorm(model_input)\n",
    "#             #denoise\n",
    "#             encoder_outputs = self.model.bert.encoder(\n",
    "#                 model_input,\n",
    "#                 attention_mask=extended_attention_mask,\n",
    "#                 head_mask=[None] * self.model.config.num_hidden_layers\n",
    "#             )\n",
    "#             sequence_output = encoder_outputs[0]\n",
    "#             prediction_scores = self.model.cls.predictions(sequence_output)\n",
    "\n",
    "#             #clamp\n",
    "#             pred = torch.argmax(prediction_scores,-1).long()\n",
    "#             denoised_word = self.model.bert.embeddings.word_embeddings(pred)\n",
    "#             # denoised_word = prediction_scores.softmax(-1) @ self.model.bert.embeddings.word_embeddings.weight.unsqueeze(0)\n",
    "\n",
    "#             # if loss_x0 == None:\n",
    "#             #     loss_x0 = F.cross_entropy(prediction_scores.view(-1, self.model.config.vocab_size),labels.flatten(),ignore_index=0)\n",
    "#             #     # loss_x0 = F.mse_loss(denoised_word, target_emb)\n",
    "#             # else:\n",
    "#             #     loss_x0 += F.cross_entropy(prediction_scores.view(-1, self.model.config.vocab_size),labels.flatten(),ignore_index=0)\n",
    "#                 # loss_x0 += F.mse_loss(denoised_word, target_emb)\n",
    "#             #DDIM\n",
    "#             alpha_tk = 1 - math.sqrt((t)/self.max_step)#+1e-5\n",
    "#             alpha_t = 1 - math.sqrt((t+1)/self.max_step)+1e-5\n",
    "#             noise = (xt - math.sqrt(alpha_t)*denoised_word)/math.sqrt(1-alpha_t)\n",
    "#             xt = math.sqrt(alpha_tk)*(xt/math.sqrt(alpha_t) + (math.sqrt((1-alpha_tk)/alpha_tk) - math.sqrt((1-alpha_t)/alpha_t))*noise)\n",
    "#             #noisy_word = math.sqrt(alpha_tk)*denoised_word + math.sqrt(1-alpha_tk)*noise\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "#         prediction_scores = self.model.cls.predictions(xt)\n",
    "#         # loss_emb = torch.norm(target_emb - xt)\n",
    "#         loss_round = F.cross_entropy(prediction_scores.view(-1, self.model.config.vocab_size),labels.flatten(),ignore_index=0)\n",
    "        \n",
    "#         loss = loss_round #/ self.max_step + loss_emb + loss_round\n",
    "#         #loss = F.smooth_l1_loss(sequence_output,word_emb)\n",
    "#         return loss,prediction_scores,labels\n",
    "\n",
    "    # def test_pretrained(self,input_ids,token_type_ids,attention_mask):\n",
    "    #     loss,prediction_scores,diffusion_steps = self.forward(input_ids,token_type_ids,attention_mask,0)\n",
    "    #     return loss,prediction_scores,diffusion_steps\n",
    "\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def sampler(self,device, batch, k=1):\n",
    "#         import time\n",
    "        \n",
    "#         start_time = time.time()\n",
    "#         inp_ids = batch['input_ids']\n",
    "#         inp_token_type_ids = batch['token_type_ids']\n",
    "#         inp_shape = inp_ids.size()\n",
    "#         N = inp_shape[0]\n",
    "#         inp_pos_ids = self.model.bert.embeddings.position_ids[:, 0 : inp_shape[1]]\n",
    "#         inp_position_embeddings = self.model.bert.embeddings.position_embeddings(inp_pos_ids)\n",
    "#         inp_emb = self.model.bert.embeddings.word_embeddings(inp_ids)\n",
    "#         # mean,std = stats\n",
    "#         # mean = torch.tensor(mean).view(1,3,1,1)\n",
    "#         # std = torch.tensor(std).view(1,3,1,1)    \n",
    "#         noisy_word = torch.normal(0,1,(N,self.max_len,self.model.config.hidden_size)).to(device) #/ math.sqrt(self.model.config.hidden_size)\n",
    "#         token_type_ids = torch.zeros(N,self.max_len).long().to(device)\n",
    "#         attention_mask = torch.ones(N,self.max_len).long().to(device)\n",
    "#         extended_attention_mask = self.model.bert.get_extended_attention_mask(attention_mask, attention_mask.shape)\n",
    "\n",
    "#         position_ids = self.model.bert.embeddings.position_ids[:, 0 : self.max_len]\n",
    "#         position_embeddings = self.model.bert.embeddings.position_embeddings(position_ids)\n",
    "#         token_type_embeddings = self.model.bert.embeddings.token_type_embeddings(token_type_ids)\n",
    "#         for t in range(self.max_step-1,0,-k):\n",
    "#         #for t in range(1999,0,-1):\n",
    "\n",
    "#             #prepare time emb\n",
    "#             diffusion_steps = torch.ones(size = (N,),device=device).long()*t\n",
    "#             time_embedding = self.time_embed(diffusion_steps).unsqueeze(1)\n",
    "\n",
    "#             model_input = inp_emb+noisy_word+inp_position_embeddings+position_embeddings+time_embedding\n",
    "#             model_input = self.model.bert.embeddings.LayerNorm(model_input)\n",
    "#             #denoise\n",
    "#             encoder_outputs = self.model.bert.encoder(\n",
    "#                 model_input,\n",
    "#                 attention_mask=extended_attention_mask,\n",
    "#                 head_mask=[None] * self.model.config.num_hidden_layers\n",
    "#             )\n",
    "#             sequence_output = encoder_outputs[0]\n",
    "#             prediction_scores = self.model.cls.predictions(sequence_output)\n",
    "\n",
    "#             #clamp\n",
    "#             pred = torch.argmax(prediction_scores,-1).long()\n",
    "#             denoised_word = self.model.bert.embeddings.word_embeddings(pred)\n",
    "#             # denoised_word = prediction_scores.softmax(-1) @ self.model.bert.embeddings.word_embeddings.weight.unsqueeze(0)\n",
    "        \n",
    "#             #DDIM\n",
    "#             alpha_tk = 1 - math.sqrt((t+1-k)/self.max_step)#+1e-5\n",
    "#             alpha_t = 1 - math.sqrt((t+1)/self.max_step)+1e-5\n",
    "#             noise = (noisy_word - math.sqrt(alpha_t)*denoised_word)/math.sqrt(1-alpha_t)\n",
    "#             noisy_word = math.sqrt(alpha_tk)*(noisy_word/math.sqrt(alpha_t) + (math.sqrt((1-alpha_tk)/alpha_tk) - math.sqrt((1-alpha_t)/alpha_t))*noise)\n",
    "#             #noisy_word = math.sqrt(alpha_tk)*denoised_word + math.sqrt(1-alpha_tk)*noise\n",
    "#             print(f\"\\rnoise level {t}  {time.time()-start_time:.2f}\",end='')\n",
    "        \n",
    "#         pred = torch.argmax(prediction_scores,-1).long()\n",
    "#         return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T22:39:43.933317Z",
     "iopub.status.busy": "2022-12-01T22:39:43.932680Z",
     "iopub.status.idle": "2022-12-01T22:39:43.961426Z",
     "shell.execute_reply": "2022-12-01T22:39:43.960945Z",
     "shell.execute_reply.started": "2022-12-01T22:39:43.933296Z"
    },
    "id": "_VfEBuKeu0c7"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load('rouge')\n",
    "    logits, labels = eval_preds\n",
    "    # print(len(logits), len(logits[0]), len(logits[0][0]), len(logits[0][0][0]))\n",
    "    # print(len(labels), len(labels[0]), len(labels[0][0]))\n",
    "    # return\n",
    "    predictions = np.argmax(logits[0], axis=-1)\n",
    "    preds = [train_set.tokenizer.decode(s) for s in predictions]\n",
    "    refs = [ train_set.tokenizer.decode(s) for s in labels]\n",
    "    # lab = np.argmax(labels, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:27.859824Z",
     "iopub.status.busy": "2022-11-30T05:29:27.859158Z",
     "iopub.status.idle": "2022-11-30T05:29:27.867136Z",
     "shell.execute_reply": "2022-11-30T05:29:27.866610Z",
     "shell.execute_reply.started": "2022-11-30T05:29:27.859801Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections.abc import Mapping\n",
    "def nested_detach(tensors):\n",
    "    \"Detach `tensors` (even if it's a nested list/tuple/dict of tensors).\"\n",
    "    if isinstance(tensors, (list, tuple)):\n",
    "        return type(tensors)(nested_detach(t) for t in tensors)\n",
    "    elif isinstance(tensors, Mapping):\n",
    "        return type(tensors)({k: nested_detach(t) for k, t in tensors.items()})\n",
    "    return tensors.detach()\n",
    "class CustomTrainer(Trainer):\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys):\n",
    "        has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        if ignore_keys is None:\n",
    "            if hasattr(self.model, \"config\"):\n",
    "                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n",
    "            else:\n",
    "                ignore_keys = []\n",
    "\n",
    "        # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.\n",
    "        if has_labels:\n",
    "            labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))\n",
    "            if len(labels) == 1:\n",
    "                labels = labels[0]\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "#             if has_labels:\n",
    "#                 with self.compute_loss_context_manager():\n",
    "#                     loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "#                 loss = loss.mean().detach()\n",
    "\n",
    "#                 if isinstance(outputs, dict):\n",
    "#                     logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + [\"loss\"])\n",
    "#                 else:\n",
    "#                     logits = outputs[1:]\n",
    "#             else:\n",
    "            loss = None\n",
    "            with self.compute_loss_context_manager():\n",
    "                outputs = model.foval(**inputs)\n",
    "            if isinstance(outputs, dict):\n",
    "                logits = tuple(v for k, v in outputs.items() if k not in ignore_keys)\n",
    "            else:\n",
    "                logits = outputs\n",
    "            # TODO: this needs to be fixed and made cleaner later.\n",
    "            if self.args.past_index >= 0:\n",
    "                self._past = outputs[self.args.past_index - 1]\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        logits = nested_detach(logits)\n",
    "        if len(logits) == 1:\n",
    "            logits = logits[0]\n",
    "\n",
    "        return (loss, logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T22:39:59.264901Z",
     "iopub.status.busy": "2022-12-01T22:39:59.264631Z",
     "iopub.status.idle": "2022-12-02T00:01:00.344201Z",
     "shell.execute_reply": "2022-12-02T00:01:00.343684Z",
     "shell.execute_reply.started": "2022-12-01T22:39:59.264881Z"
    },
    "id": "5ECm2OvglybP",
    "outputId": "f0d730ab-9ab6-4a4d-fce3-c99c7998b730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1600\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mquangminhdinh\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/diff-lm/wandb/run-20221201_224001-1g45fbkw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/quangminhdinh/diff_lm_semantic_parsing/runs/1g45fbkw\" target=\"_blank\">true-diff-final-170</a></strong> to <a href=\"https://wandb.ai/quangminhdinh/diff_lm_semantic_parsing\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 1:20:47, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>15.526800</td>\n",
       "      <td>15.349535</td>\n",
       "      <td>0.012917</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.012587</td>\n",
       "      <td>0.012568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14.829100</td>\n",
       "      <td>14.845671</td>\n",
       "      <td>0.016261</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.015530</td>\n",
       "      <td>0.015471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>14.352200</td>\n",
       "      <td>14.054068</td>\n",
       "      <td>0.029379</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.026392</td>\n",
       "      <td>0.026377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12.942100</td>\n",
       "      <td>12.695997</td>\n",
       "      <td>0.048673</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.042313</td>\n",
       "      <td>0.042287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>11.743100</td>\n",
       "      <td>11.668353</td>\n",
       "      <td>0.055378</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.050236</td>\n",
       "      <td>0.050262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>11.007600</td>\n",
       "      <td>10.834443</td>\n",
       "      <td>0.056188</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.051883</td>\n",
       "      <td>0.051899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>10.459800</td>\n",
       "      <td>10.221531</td>\n",
       "      <td>0.057022</td>\n",
       "      <td>0.005448</td>\n",
       "      <td>0.053524</td>\n",
       "      <td>0.053503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9.926200</td>\n",
       "      <td>9.738289</td>\n",
       "      <td>0.058515</td>\n",
       "      <td>0.005732</td>\n",
       "      <td>0.054436</td>\n",
       "      <td>0.054418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9.536800</td>\n",
       "      <td>9.388222</td>\n",
       "      <td>0.060949</td>\n",
       "      <td>0.005957</td>\n",
       "      <td>0.055610</td>\n",
       "      <td>0.055628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>9.361500</td>\n",
       "      <td>9.133805</td>\n",
       "      <td>0.064776</td>\n",
       "      <td>0.006080</td>\n",
       "      <td>0.058312</td>\n",
       "      <td>0.058286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>9.174500</td>\n",
       "      <td>8.918622</td>\n",
       "      <td>0.068592</td>\n",
       "      <td>0.006750</td>\n",
       "      <td>0.060798</td>\n",
       "      <td>0.060795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>9.037700</td>\n",
       "      <td>8.784188</td>\n",
       "      <td>0.070436</td>\n",
       "      <td>0.006497</td>\n",
       "      <td>0.062620</td>\n",
       "      <td>0.062608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>8.741400</td>\n",
       "      <td>8.632365</td>\n",
       "      <td>0.072207</td>\n",
       "      <td>0.006527</td>\n",
       "      <td>0.064138</td>\n",
       "      <td>0.064134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>8.722400</td>\n",
       "      <td>8.511700</td>\n",
       "      <td>0.073877</td>\n",
       "      <td>0.006566</td>\n",
       "      <td>0.066109</td>\n",
       "      <td>0.066087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>8.714700</td>\n",
       "      <td>8.438259</td>\n",
       "      <td>0.074179</td>\n",
       "      <td>0.006659</td>\n",
       "      <td>0.066267</td>\n",
       "      <td>0.066237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>8.394000</td>\n",
       "      <td>8.397321</td>\n",
       "      <td>0.074904</td>\n",
       "      <td>0.006551</td>\n",
       "      <td>0.067228</td>\n",
       "      <td>0.067204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>8.488400</td>\n",
       "      <td>8.310378</td>\n",
       "      <td>0.075183</td>\n",
       "      <td>0.006495</td>\n",
       "      <td>0.067522</td>\n",
       "      <td>0.067537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>8.354100</td>\n",
       "      <td>8.294708</td>\n",
       "      <td>0.075685</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.067818</td>\n",
       "      <td>0.067824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>8.488500</td>\n",
       "      <td>8.253779</td>\n",
       "      <td>0.075809</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>0.067892</td>\n",
       "      <td>0.067884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>8.298900</td>\n",
       "      <td>8.278633</td>\n",
       "      <td>0.075826</td>\n",
       "      <td>0.006689</td>\n",
       "      <td>0.068347</td>\n",
       "      <td>0.068319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5abfea607184b2f9e293531911bb8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./true-diff-final-170/checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./true-diff-final-170/checkpoint-1000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./true-diff-final-170/checkpoint-1500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./true-diff-final-170/checkpoint-2000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e33424db8dd419fa99de450e6d19098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▇▅▄▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/rouge1</td><td>▁▁▃▅▆▆▆▆▆▇▇▇████████</td></tr><tr><td>eval/rouge2</td><td>▁▁▁▂▄▆▇▇▇▇██████████</td></tr><tr><td>eval/rougeL</td><td>▁▁▃▅▆▆▆▆▆▇▇▇▇███████</td></tr><tr><td>eval/rougeLsum</td><td>▁▁▃▅▆▆▆▆▆▇▇▇▇███████</td></tr><tr><td>eval/runtime</td><td>▂▁▁▅▂█▄▇▆▇▅▅▄█▆▆▅▆█▇</td></tr><tr><td>eval/samples_per_second</td><td>▇██▄▇▁▅▂▃▂▄▄▅▁▃▃▄▃▁▂</td></tr><tr><td>eval/steps_per_second</td><td>▇██▅▇▁▅▂▃▂▅▄▅▁▃▃▄▃▁▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▁▂▃▃▄▅▆▆▇████▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▇▇▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>8.27863</td></tr><tr><td>eval/rouge1</td><td>0.07583</td></tr><tr><td>eval/rouge2</td><td>0.00669</td></tr><tr><td>eval/rougeL</td><td>0.06835</td></tr><tr><td>eval/rougeLsum</td><td>0.06832</td></tr><tr><td>eval/runtime</td><td>72.1565</td></tr><tr><td>eval/samples_per_second</td><td>25.93</td></tr><tr><td>eval/steps_per_second</td><td>0.416</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>2000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>8.2989</td></tr><tr><td>train/total_flos</td><td>0.0</td></tr><tr><td>train/train_loss</td><td>10.47307</td></tr><tr><td>train/train_runtime</td><td>4851.506</td></tr><tr><td>train/train_samples_per_second</td><td>6.596</td></tr><tr><td>train/train_steps_per_second</td><td>0.412</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">true-diff-final-170</strong>: <a href=\"https://wandb.ai/quangminhdinh/diff_lm_semantic_parsing/runs/1g45fbkw\" target=\"_blank\">https://wandb.ai/quangminhdinh/diff_lm_semantic_parsing/runs/1g45fbkw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221201_224001-1g45fbkw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_len = 64\n",
    "diff_step = 170\n",
    "initializing = 'base/bert-tiny'#'base/bert-mini'\n",
    "checkpoint = 'true-diff/checkpoint-2000'\n",
    "device = torch.device('cuda')\n",
    "model = diffusion_bert(initializing,max_len,diff_step)\n",
    "state = torch.load(initializing+'/pytorch_model.bin', map_location=device) #\"/Saved_Models/20220903bert_diffusion/bestloss.pkl\")\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "# 1. filter out unnecessary keys\n",
    "if list(state.keys())[0].startswith(\"module.\"):\n",
    "    state = {k[7:]: v for k, v in state.items() if k[7:] in model_dict}\n",
    "else:\n",
    "    state = {k: v for k, v in state.items() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(state)\n",
    "# 3. load the new state dict\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "# model.load_state_dict(state,strict=True)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Trial 31\")\n",
    "\n",
    "train_set = OvernightDataset(train_dict, init_model=initializing, max_len=max_len)\n",
    "val_set = OvernightDataset(dev_dict, init_model=initializing, max_len=max_len)\n",
    "test_set = OvernightDataset(test_all, init_model=initializing, max_len=max_len)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_set,         # training dataset\n",
    "    eval_dataset=val_set,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"Start decoding\")\n",
    "\n",
    "# out = model.sampler(device, 10, 128)\n",
    "# with open(\"samples.txt\", 'w', encoding=\"utf-8\") as f:\n",
    "#     for s in out:\n",
    "#         sample = test_set.tokenizer.decode(s.cpu().flatten())\n",
    "#         f.write(sample+\"\\n\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-30T03:12:01.385813Z",
     "iopub.status.busy": "2022-11-30T03:12:01.384966Z",
     "iopub.status.idle": "2022-11-30T03:12:01.423529Z",
     "shell.execute_reply": "2022-11-30T03:12:01.423023Z",
     "shell.execute_reply.started": "2022-11-30T03:12:01.385788Z"
    },
    "id": "TfQKO5Z5DHTk",
    "outputId": "72ca6284-0272-4bc2-9adc-799b6048387d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 3116, 3005,  ...,    0,    0,    0],\n",
       "        [ 101, 3116, 3005,  ...,    0,    0,    0],\n",
       "        [ 101, 3116, 3005,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 3116, 3005,  ...,    0,    0,    0],\n",
       "        [ 101, 3116, 3005,  ...,    0,    0,    0],\n",
       "        [ 101, 3116, 3005,  ...,    0,    0,    0]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataloader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "batch = next(iter(test_dataloader))\n",
    "for key, value in batch.items():\n",
    "    batch[key] = batch[key].to(device)\n",
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "execution": {
     "iopub.execute_input": "2022-12-02T00:08:39.020310Z",
     "iopub.status.busy": "2022-12-02T00:08:39.020038Z",
     "iopub.status.idle": "2022-12-02T00:10:37.330714Z",
     "shell.execute_reply": "2022-12-02T00:10:37.330218Z",
     "shell.execute_reply.started": "2022-12-02T00:08:39.020284Z"
    },
    "id": "0NJMY_Tw8x4c",
    "outputId": "baee300d-beef-4062-da05-aeb602770548"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2740\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='43' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43/43 05:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=(array([[[ -4.898205  ,  -4.515242  ,  -5.566231  , ...,  -5.113649  ,\n",
       "          -6.22801   ,  -9.755732  ],\n",
       "        [ -4.7080307 ,  -4.8124537 ,  -5.517356  , ...,  -5.985125  ,\n",
       "          -6.440933  , -11.015397  ],\n",
       "        [ -4.9037647 ,  -5.769612  ,  -5.762281  , ...,  -6.3659887 ,\n",
       "          -6.847018  ,  -7.334628  ],\n",
       "        ...,\n",
       "        [ -8.045854  ,  -8.809026  ,  -9.20293   , ..., -10.893935  ,\n",
       "         -10.213099  , -11.537753  ],\n",
       "        [ -8.121555  ,  -8.3441515 ,  -9.676844  , ...,  -9.980815  ,\n",
       "          -9.632717  , -12.442889  ],\n",
       "        [ -5.500577  ,  -5.040684  ,  -5.073077  , ...,  -6.2018003 ,\n",
       "          -6.398684  ,  -8.676195  ]],\n",
       "\n",
       "       [[ -1.453227  ,  -1.3636385 ,  -1.7060838 , ...,  -2.4780207 ,\n",
       "          -3.263864  ,  -5.6988497 ],\n",
       "        [ -0.67650926,  -1.2274568 ,  -1.0702784 , ...,  -2.6252246 ,\n",
       "          -2.7142606 ,  -5.723042  ],\n",
       "        [ -7.035838  ,  -6.83175   ,  -8.184712  , ...,  -9.846008  ,\n",
       "          -7.7568445 , -10.246799  ],\n",
       "        ...,\n",
       "        [ -0.76872146,  -1.0961486 ,  -2.19747   , ...,  -4.4138217 ,\n",
       "          -3.3947356 ,  -9.917117  ],\n",
       "        [ -6.984329  ,  -6.6080127 ,  -7.3110743 , ...,  -7.634713  ,\n",
       "          -7.7806306 , -11.396313  ],\n",
       "        [ -9.189369  ,  -9.802048  ,  -9.900036  , ...,  -9.920853  ,\n",
       "          -9.367967  , -11.055305  ]],\n",
       "\n",
       "       [[ -6.637102  ,  -5.7367835 ,  -7.342643  , ...,  -7.1375103 ,\n",
       "          -8.350229  ,  -9.751477  ],\n",
       "        [ -5.892396  ,  -5.6791315 ,  -7.4583297 , ...,  -7.8114176 ,\n",
       "          -7.789742  , -13.401466  ],\n",
       "        [-10.499216  , -10.107776  , -11.961958  , ..., -11.620149  ,\n",
       "         -12.341577  , -13.818754  ],\n",
       "        ...,\n",
       "        [ -5.262212  ,  -4.8214855 ,  -5.7509584 , ...,  -6.645602  ,\n",
       "          -6.621612  ,  -8.673854  ],\n",
       "        [ -8.14617   ,  -7.332765  ,  -7.638058  , ...,  -8.200926  ,\n",
       "          -8.910286  ,  -9.474124  ],\n",
       "        [ -6.4587164 ,  -6.5627937 ,  -6.411495  , ...,  -7.171233  ,\n",
       "          -7.1678076 , -10.027859  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ -3.2376728 ,  -2.646062  ,  -3.4581273 , ...,  -4.952792  ,\n",
       "          -5.0797358 ,  -6.28246   ],\n",
       "        [ -4.509985  ,  -5.265135  ,  -5.6350055 , ...,  -6.61747   ,\n",
       "          -4.959988  , -10.010248  ],\n",
       "        [ -4.7761345 ,  -4.571862  ,  -5.182709  , ...,  -5.7424574 ,\n",
       "          -5.813352  ,  -7.8449564 ],\n",
       "        ...,\n",
       "        [ -8.023149  ,  -9.304086  , -10.195738  , ..., -10.488722  ,\n",
       "          -9.57683   , -13.356202  ],\n",
       "        [ -8.960609  ,  -9.620618  ,  -9.540854  , ..., -10.927571  ,\n",
       "          -9.540402  , -10.387088  ],\n",
       "        [ -8.413072  ,  -8.195572  ,  -8.292324  , ...,  -9.4082    ,\n",
       "          -8.392336  , -13.138272  ]],\n",
       "\n",
       "       [[ -7.5225945 ,  -6.8506327 ,  -7.54336   , ...,  -9.80659   ,\n",
       "          -8.730526  ,  -7.9669003 ],\n",
       "        [ -5.3107166 ,  -5.217307  ,  -5.5532837 , ...,  -7.331361  ,\n",
       "          -6.7396855 , -11.920434  ],\n",
       "        [ -4.1348896 ,  -3.779549  ,  -4.2841578 , ...,  -6.754587  ,\n",
       "          -5.5272117 ,  -8.9931965 ],\n",
       "        ...,\n",
       "        [ -7.0742064 ,  -6.311     ,  -7.3006864 , ...,  -8.127996  ,\n",
       "          -7.8534966 , -11.148785  ],\n",
       "        [ -4.385206  ,  -5.0516176 ,  -5.6327333 , ...,  -5.186936  ,\n",
       "          -5.911897  ,  -8.338327  ],\n",
       "        [ -9.053491  , -10.13482   , -10.263395  , ..., -12.837766  ,\n",
       "         -10.723432  , -15.742945  ]],\n",
       "\n",
       "       [[ -1.1066974 ,  -1.2028012 ,  -2.7867033 , ...,  -2.3976893 ,\n",
       "          -3.5267134 , -10.263418  ],\n",
       "        [ -4.546466  ,  -5.4923544 ,  -5.54685   , ...,  -6.073861  ,\n",
       "          -6.0181737 ,  -9.984922  ],\n",
       "        [ -4.760625  ,  -4.6887693 ,  -5.743285  , ...,  -5.7034636 ,\n",
       "          -5.529112  ,  -7.255716  ],\n",
       "        ...,\n",
       "        [ -9.634449  ,  -9.346433  , -10.634798  , ..., -11.935718  ,\n",
       "         -10.914684  , -12.539271  ],\n",
       "        [-10.418589  , -10.3819275 , -10.716883  , ..., -11.647388  ,\n",
       "         -11.55898   , -14.299028  ],\n",
       "        [ -4.0569434 ,  -4.918004  ,  -5.1412616 , ...,  -7.164296  ,\n",
       "          -5.6542172 ,  -7.825648  ]]], dtype=float32), array([[ 101, 2711, 2008, ...,    0,    0,    0],\n",
       "       [ 101, 3116, 3005, ...,    0,    0,    0],\n",
       "       [ 101, 3116, 2008, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 101, 2711, 2008, ...,    0,    0,    0],\n",
       "       [ 101, 3105, 2516, ...,    0,    0,    0],\n",
       "       [ 101, 3076, 3005, ...,    0,    0,    0]])), label_ids=array([[ 101, 2711, 2008, ...,    0,    0,    0],\n",
       "       [ 101, 3116, 3005, ...,    0,    0,    0],\n",
       "       [ 101, 3116, 2008, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 101, 2711, 2008, ...,    0,    0,    0],\n",
       "       [ 101, 3105, 2516, ...,    0,    0,    0],\n",
       "       [ 101, 3076, 3005, ...,    0,    0,    0]]), metrics={'test_loss': 8.312190055847168, 'test_rouge1': 0.07559003491348604, 'test_rouge2': 0.0067705539314500945, 'test_rougeL': 0.06825427736011358, 'test_rougeLsum': 0.06826998237302323, 'test_runtime': 118.2981, 'test_samples_per_second': 23.162, 'test_steps_per_second': 0.363})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T00:13:05.282895Z",
     "iopub.status.busy": "2022-12-02T00:13:05.282625Z",
     "iopub.status.idle": "2022-12-02T00:14:45.007845Z",
     "shell.execute_reply": "2022-12-02T00:14:45.007301Z",
     "shell.execute_reply.started": "2022-12-02T00:13:05.282875Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 168\n",
      "  Batch size = 64\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 391\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calendar : {'test_loss': 8.21792221069336, 'test_rouge1': 0.06783581356972464, 'test_rouge2': 0.0045402553508246245, 'test_rougeL': 0.06032255768718671, 'test_rougeLsum': 0.06031259060720835, 'test_runtime': 6.1413, 'test_samples_per_second': 27.356, 'test_steps_per_second': 0.488}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 399\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basketball : {'test_loss': 8.02482795715332, 'test_rouge1': 0.08424357310964287, 'test_rouge2': 0.00068388263428498, 'test_rougeL': 0.07239258454819164, 'test_rougeLsum': 0.07235853776563163, 'test_runtime': 13.9203, 'test_samples_per_second': 28.088, 'test_steps_per_second': 0.503}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 189\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks : {'test_loss': 7.479801177978516, 'test_rouge1': 0.0884600613646698, 'test_rouge2': 0.011811837842164474, 'test_rougeL': 0.08005419050726098, 'test_rougeLsum': 0.08013123123130933, 'test_runtime': 14.246, 'test_samples_per_second': 28.008, 'test_steps_per_second': 0.491}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 161\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "housing : {'test_loss': 9.14459228515625, 'test_rouge1': 0.07042228241661184, 'test_rouge2': 0.005688391554043597, 'test_rougeL': 0.06416416427697036, 'test_rougeLsum': 0.064200116634095, 'test_runtime': 6.515, 'test_samples_per_second': 29.01, 'test_steps_per_second': 0.46}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 216\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publications : {'test_loss': 9.366532325744629, 'test_rouge1': 0.07365486754967786, 'test_rouge2': 0.006461402857951025, 'test_rougeL': 0.06717521711296856, 'test_rougeLsum': 0.06724217523165307, 'test_runtime': 6.0529, 'test_samples_per_second': 26.599, 'test_steps_per_second': 0.496}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipes : {'test_loss': 8.897368431091309, 'test_rouge1': 0.05899093856022335, 'test_rouge2': 0.0033138579113045413, 'test_rougeL': 0.05401114719701902, 'test_rougeLsum': 0.05400464527974243, 'test_runtime': 7.9368, 'test_samples_per_second': 27.215, 'test_steps_per_second': 0.504}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 884\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurants : {'test_loss': 8.255416870117188, 'test_rouge1': 0.0764755262219089, 'test_rouge2': 0.010116724522269457, 'test_rougeL': 0.06908001325285014, 'test_rougeLsum': 0.06904295824151858, 'test_runtime': 11.9793, 'test_samples_per_second': 27.714, 'test_steps_per_second': 0.501}\n",
      "\n",
      "socialnetwork : {'test_loss': 8.346147537231445, 'test_rouge1': 0.07200017545358953, 'test_rouge2': 0.007280928048368611, 'test_rougeL': 0.06702700877327436, 'test_rougeLsum': 0.06707011919201951, 'test_runtime': 32.0817, 'test_samples_per_second': 27.555, 'test_steps_per_second': 0.436}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_domain_data(data, domain):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    domain_data = data[domain]\n",
    "    for record in domain_data:\n",
    "        inputs.append(record['natural'])\n",
    "        outputs.append(record['canonical'])\n",
    "    return inputs, outputs\n",
    "\n",
    "for dom in DOMAINS:\n",
    "    test_dom = OvernightDataset(test_all, init_model=initializing, max_len=max_len, func=lambda data : prepare_domain_data(data, dom))\n",
    "    ret = trainer.predict(test_dom)\n",
    "    print(dom, \":\", ret.metrics)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-27T00:14:50.432725Z",
     "iopub.status.busy": "2022-11-27T00:14:50.431908Z",
     "iopub.status.idle": "2022-11-27T00:14:57.575719Z",
     "shell.execute_reply": "2022-11-27T00:14:57.575166Z",
     "shell.execute_reply.started": "2022-11-27T00:14:50.432699Z"
    },
    "id": "ulArnu89DABD",
    "outputId": "20f579f9-a8fb-4799-d492-58dd57ea1ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  3.5040\n",
      "viaduct donetsk conversion today pitched blocks witness zombie sensoryeumhip nexus bears semi psalm hop fearchester sec tuition • termhausbaum fall sessions middlesex rosen counselorße student com triernot minimum recording socrates device federaltor ¹straße trees sip logdorf bounds mileiman showers aa reference sec mixtape faith ¹ loft bounds https kickoff courtroomcaster bonusacies\n",
      "ot: se is, book is people is back of and other史. named [SEP] that won is ) home of that. use way of and is. is. her this of where you, and a a, is. this a and is, s is is. \" ) it it major center, whose., two\n",
      "org: [CLS] meeting whose start time is smaller than 10am [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "wins correspondence vertex project datctumctum duet filipino cameo styleutive battalionctum tourtist boys musician stranger skater assignments village constituent assignmentsaday assignments annviewpal clap mahal quaker traveler nomination excel vs peek sleep assignments recording cruiser seater seating vienna jury busy swimmer g tap images comparison baseline knee hospital rustic soloist sailor decent witnessgui cap v type exams\n",
      "ot: time home —.... is is of car a of. was review was of. a. after. like,. not.. average with of is american. is two is. club ) the that march family is right. about up is hand, the. up. ). population is her is with\n",
      "org: [CLS] meeting whose date is at least jan 2 or jan 3 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "db penis male story hughes rating minimum hypothetical 2000 candidate potassium risk [UNK] usaf deviation postponed minimum galactic deviation category horse starter minimum ‡ note mas optional minimum elliptic 1962 no doi register timeline sax term association silent performance magnetic choice center 2010 myanmar alexandramined fuel vacancy scheduling life shop theatre vip starter conductor minimum lcd stool football sec variant ‡ perennial standard\n",
      "ot: ( family. was most addition, of that is whereer. ),. that in. not,. ) that and of or. now,,. ( - living s post once.,, not land [SEP],. time. which, and of [. village.,. of that is is. described\n",
      "org: [CLS] meeting whose date is jan 2 and whose date is jan 2 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "trustees member foundrytime il wilkinsonbas swap housing bold academy caps terms kurdistan planet pile token rebel delegate mega hq trusteespace trustees representativeᵈ pub sec hq listening provider 1968 drive congressional officer income peninsula adrian capita township panchayat shady presidents kilometer yiddish loan squad dong spaceship biography panchayatmont trustees bilingual committees trustees list unionuaries crusader brigade district awards qi\n",
      "ot: ; a. has of. team... has the ( and where film district or., is [SEP] of, that is is known., the may is that table, may that what population., that riding use., other fed living other all is is of \", and is living that family that baseball\n",
      "org: [CLS] meeting whose end time is smaller than end time of weekly standup [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "##ignment hospital fabrication hill vacancy washington noble cartridge referencefer criteria athlete minimumtionignment minors nasa mega subject host mammal singapore raceway umpire number vacancy transmitter borg trailer vacancylov rockgenic vacancy microwave neighbor offenders ¹ spinal spouseference percentageignmentstation normchule isil hitignment wrestler translatorlity 1 metallity raceway vacancy semiconductor idahoignment sectional duo magnet global\n",
      "ot: bit and, of is is the. [SEP] is of of number is sq is, is family is, state,, is. is who side ( [SEP] ines is,, march which has ( name, a that of., ) center. she is he and,. in a, ) family \". track\n",
      "org: [CLS] meeting that has the most number of attendee [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "out = model.sampler(device, batch)\n",
    "_, otpred, _ = model(**batch)\n",
    "oot = torch.argmax(otpred,-1).long()\n",
    "for i, s in enumerate(out[5:10]):\n",
    "    sample = test_set.tokenizer.decode(s.cpu().flatten())\n",
    "    org = test_set.tokenizer.decode(batch['labels'][i].cpu().flatten())\n",
    "    ot = test_set.tokenizer.decode(oot[i].cpu().flatten())\n",
    "    print()\n",
    "    print(sample)\n",
    "    print(\"ot:\", ot)\n",
    "    print(\"org:\", org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7AEhL9lt5Bg",
    "outputId": "9ede9a1d-3f8b-4a20-9c81-914092b411ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "gtqgW_M80Cf_",
    "outputId": "a6462baa-03e0-4959-b5bb-9371a9b4f9e9"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-09307355aaeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0memp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-758da7c56dd4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels, t)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mposition_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  emp = test_set.__getitem__(0)\n",
    "  outputs = model(emp['input_ids'], emp['token_type_ids'], emp['attention_mask'], emp['labels'])\n",
    "outputs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
