{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T13:46:55.293849Z",
     "iopub.status.busy": "2022-11-26T13:46:55.292668Z",
     "iopub.status.idle": "2022-11-26T13:47:08.971699Z",
     "shell.execute_reply": "2022-11-26T13:47:08.970336Z",
     "shell.execute_reply.started": "2022-11-26T13:46:55.293777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.23.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.5.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.64.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2022.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.13)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.8.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.3.2)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (8.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.10)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.14.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (18.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from rouge-score) (1.1.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from rouge-score) (3.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rouge-score) (1.23.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge-score) (1.14.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (2022.7.9)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=a247dd822b715c75ac0f724676ab393c1057156c59ddfabc4ac0bd6fbb55dba2\n",
      "  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.20.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.1)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.28.1)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.1.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Building wheels for collected packages: promise, pathtools\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=0345d31828902b80e204abd1ea11f36f1ad14bd90ff68d8f00c073a85f03b057\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/e8/83/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=df0d93bf0776fbc184440fecb70dd20df3adea46fc85eb3a678cf34061137a7f\n",
      "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built promise pathtools\n",
      "Installing collected packages: pathtools, urllib3, shortuuid, setproctitle, promise, docker-pycreds, sentry-sdk, wandb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.10\n",
      "    Uninstalling urllib3-1.26.10:\n",
      "      Successfully uninstalled urllib3-1.26.10\n",
      "Successfully installed docker-pycreds-0.4.0 pathtools-0.1.2 promise-2.3 sentry-sdk-1.11.1 setproctitle-1.3.2 shortuuid-1.0.11 urllib3-1.26.13 wandb-0.13.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install evaluate\n",
    "%pip install rouge-score\n",
    "%pip install transformers\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eBEmrRCLx1iY",
    "outputId": "7eb461e0-557f-4415-8b76-1a1349bd07b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i60EEs-QzNoY",
    "outputId": "8d8e3b06-d7c2-4601-c94c-3a307de16651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/diff-lm\n",
      "\u001b[0m\u001b[01;34mbase\u001b[0m/  \u001b[01;34mlogs\u001b[0m/                 \u001b[01;34mmodelstabd\u001b[0m/    samples.txt\n",
      "\u001b[01;34mdata\u001b[0m/  \u001b[01;34mmodels-base-uncased\u001b[0m/  \u001b[01;34mmodelstabd-1\u001b[0m/  \u001b[01;34mwandb\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cd drive/MyDrive/diff-lm/\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T13:47:24.390522Z",
     "iopub.status.busy": "2022-11-26T13:47:24.389971Z",
     "iopub.status.idle": "2022-11-26T13:47:24.395268Z",
     "shell.execute_reply": "2022-11-26T13:47:24.394369Z",
     "shell.execute_reply.started": "2022-11-26T13:47:24.390500Z"
    },
    "id": "B2HA6Vil3Ihb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# os.listdir('base')\n",
    "# data = []\n",
    "# with open(\"data/calendar.dev.jsonl\") as f:\n",
    "#     for line in f:\n",
    "#         a=json.loads(line)\n",
    "#         a[\"formula\"] = a[\"formula\"].replace(\"edu.stanford.nlp.sempre.overnight.SimpleWorld.\", \"\")\n",
    "#         data.append(a)\n",
    "\n",
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-26T13:47:27.892287Z",
     "iopub.status.busy": "2022-11-26T13:47:27.891322Z",
     "iopub.status.idle": "2022-11-26T13:47:28.194902Z",
     "shell.execute_reply": "2022-11-26T13:47:28.194199Z",
     "shell.execute_reply.started": "2022-11-26T13:47:27.892265Z"
    },
    "id": "UEG_5Kfm4gmf",
    "outputId": "902ab4ff-0f80-4641-f7aa-06eda3f9ff1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'canonical': 'meeting whose end time is larger than 10am or 3pm',\n",
       " 'formula': '(call listValue (call filter (call getProperty (call singleton en.meeting) (string !type)) (call ensureNumericProperty (string end_time)) (string >) (call ensureNumericEntity (call concat (time 10 0) (time 15 0)))))',\n",
       " 'natural': 'which meetings end later than 10 in the morning or 3 in the afternoon'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOMAINS = (\n",
    "    \"calendar\",\n",
    "    \"basketball\",\n",
    "    \"blocks\",\n",
    "    \"housing\",\n",
    "    \"publications\",\n",
    "    \"recipes\",\n",
    "    \"restaurants\",\n",
    "    \"socialnetwork\",\n",
    ")\n",
    "\n",
    "def get_data(domain, dataset=\"train_with_dev\"):\n",
    "    data = []\n",
    "    with open(\"data/\" + domain + \".\" + dataset + \".jsonl\") as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line)\n",
    "            record[\"formula\"] = simplifier(record[\"formula\"])\n",
    "            data.append(record)\n",
    "    return data\n",
    "\n",
    "simplifier = lambda txt: txt.replace(\"edu.stanford.nlp.sempre.overnight.SimpleWorld.\", \"\")\n",
    "train_all = {}\n",
    "test_all = {}\n",
    "for domain in DOMAINS:\n",
    "    train_all[domain] = get_data(domain)\n",
    "    test_all[domain] = get_data(domain, dataset=\"test\")\n",
    "\n",
    "# train_all[DOMAINS[0]][0]\n",
    "# dev_all[DOMAINS[0]][0]\n",
    "test_all[DOMAINS[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-26T13:47:32.119529Z",
     "iopub.status.busy": "2022-11-26T13:47:32.119226Z",
     "iopub.status.idle": "2022-11-26T13:47:32.211180Z",
     "shell.execute_reply": "2022-11-26T13:47:32.210640Z",
     "shell.execute_reply.started": "2022-11-26T13:47:32.119511Z"
    },
    "id": "8-Dv08Cf8XbJ",
    "outputId": "3d421ba0-874e-4253-a8d6-fc9e222f05cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_train_dev(domain, domains_data, train_size=200, remain_dev=0.2, shuffle=True):\n",
    "  data = domains_data[domain]\n",
    "  if shuffle:\n",
    "    np.random.shuffle(data)\n",
    "  size = len(data)\n",
    "  dev_size = np.ceil((size - train_size) * 0.2).astype(int) + train_size\n",
    "  return data[:train_size], data[train_size:dev_size]\n",
    "\n",
    "train_dict = {}\n",
    "dev_dict = {}\n",
    "for domain in DOMAINS:\n",
    "  train_dict[domain], dev_dict[domain] = split_train_dev(domain, train_all)\n",
    "\n",
    "len(train_dict[DOMAINS[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T13:47:34.322966Z",
     "iopub.status.busy": "2022-11-26T13:47:34.322035Z",
     "iopub.status.idle": "2022-11-26T13:47:34.327727Z",
     "shell.execute_reply": "2022-11-26T13:47:34.326967Z",
     "shell.execute_reply.started": "2022-11-26T13:47:34.322944Z"
    },
    "id": "6kqoTd7OgBlq"
   },
   "outputs": [],
   "source": [
    "def prepare_data(data, shuffle=True):\n",
    "  inputs = []\n",
    "  outputs = []\n",
    "  for domain in DOMAINS:\n",
    "    domain_data = data[domain]\n",
    "    if shuffle:\n",
    "      np.random.shuffle(domain_data)\n",
    "    for record in domain_data:\n",
    "      inputs.append(record['natural'])\n",
    "      outputs.append(record['canonical'])\n",
    "  return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T13:47:36.680140Z",
     "iopub.status.busy": "2022-11-26T13:47:36.679387Z",
     "iopub.status.idle": "2022-11-26T13:47:44.409052Z",
     "shell.execute_reply": "2022-11-26T13:47:44.408414Z",
     "shell.execute_reply.started": "2022-11-26T13:47:36.680117Z"
    },
    "id": "Q1Cn4rmp3D9G"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import csv\n",
    "from transformers import AutoModelForPreTraining,AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "execution": {
     "iopub.execute_input": "2022-11-26T13:47:50.397891Z",
     "iopub.status.busy": "2022-11-26T13:47:50.397017Z",
     "iopub.status.idle": "2022-11-26T13:47:53.310065Z",
     "shell.execute_reply": "2022-11-26T13:47:53.308987Z",
     "shell.execute_reply.started": "2022-11-26T13:47:50.397870Z"
    },
    "id": "fdeCmFvGMnnm",
    "outputId": "6f388614-c924-4fb6-fd9f-654d778f0a5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T13:47:48.358060Z",
     "iopub.status.busy": "2022-11-26T13:47:48.356736Z",
     "iopub.status.idle": "2022-11-26T13:47:48.362844Z",
     "shell.execute_reply": "2022-11-26T13:47:48.362128Z",
     "shell.execute_reply.started": "2022-11-26T13:47:48.358037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=diff_lm_semantic_parsing\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=diff_lm_semantic_parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T13:48:14.297985Z",
     "iopub.status.busy": "2022-11-26T13:48:14.296388Z",
     "iopub.status.idle": "2022-11-26T13:48:14.310034Z",
     "shell.execute_reply": "2022-11-26T13:48:14.309037Z",
     "shell.execute_reply.started": "2022-11-26T13:48:14.297943Z"
    },
    "id": "KX-v3wbi3EuP"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to = 'wandb',  \n",
    "    run_name=\"noise-multiplier-predictor-loss123-scratch\",\n",
    "    output_dir='./modelsloss123-scratch',          # output directory\n",
    "    num_train_epochs=20,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    label_names=['labels'],\n",
    "    eval_accumulation_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T13:48:20.373257Z",
     "iopub.status.busy": "2022-11-26T13:48:20.371811Z",
     "iopub.status.idle": "2022-11-26T13:48:20.380529Z",
     "shell.execute_reply": "2022-11-26T13:48:20.379324Z",
     "shell.execute_reply.started": "2022-11-26T13:48:20.373233Z"
    },
    "id": "_T0QrAntYAaT"
   },
   "outputs": [],
   "source": [
    "class OvernightDataset(Dataset): \n",
    "    def __init__(self, data, init_model, max_len):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(init_model)\n",
    "        self.inputs, self.labels = prepare_data(data)\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer.model_max_length = max_len\n",
    "    def __getitem__(self, index):\n",
    "        from_tokenizer = self.tokenizer(self.inputs[index],padding=\"max_length\",truncation = True,return_tensors=\"pt\")\n",
    "        label_tokens = self.tokenizer(self.labels[index],padding=\"max_length\",truncation = True,return_tensors=\"pt\")\n",
    "        input_ids = from_tokenizer[\"input_ids\"].squeeze_().long()\n",
    "        ret_labels = label_tokens[\"input_ids\"].squeeze_().long()\n",
    "        token_type_ids = from_tokenizer[\"token_type_ids\"].squeeze_().long()\n",
    "        attention_mask = from_tokenizer[\"attention_mask\"].squeeze_().long()\n",
    "        labels_token_type_ids = label_tokens[\"token_type_ids\"].squeeze_().long()\n",
    "        labels_attention_mask = label_tokens[\"attention_mask\"].squeeze_().long()\n",
    "        # return input_ids,token_type_ids,attention_mask\n",
    "        return {\"input_ids\": input_ids, \n",
    "                \"token_type_ids\" : token_type_ids, \n",
    "                \"attention_mask\" : attention_mask, \n",
    "                \"labels\" : ret_labels, \n",
    "                \"labels_token_type_ids\" : labels_token_type_ids, \n",
    "                \"labels_attention_mask\" : labels_attention_mask}\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T13:48:23.219362Z",
     "iopub.status.busy": "2022-11-26T13:48:23.218796Z",
     "iopub.status.idle": "2022-11-26T13:48:23.243907Z",
     "shell.execute_reply": "2022-11-26T13:48:23.243185Z",
     "shell.execute_reply.started": "2022-11-26T13:48:23.219342Z"
    },
    "id": "OcUyys55lVWo"
   },
   "outputs": [],
   "source": [
    "class diffusion_bert(nn.Module):\n",
    "    def __init__(self,init_model,max_len,max_step,k=1, rng_max=8) -> None:\n",
    "        super().__init__()\n",
    "        if \"bert-base\" in init_model:\n",
    "            self.model = AutoModelForMaskedLM.from_pretrained(init_model)\n",
    "            freezed_w = [self.model.bert.embeddings.token_type_embeddings.weight,self.model.bert.embeddings.word_embeddings.weight] #self.model.bert.embeddings.LayerNorm.weight, self.model.bert.embeddings.LayerNorm.bias\n",
    "        else:\n",
    "            self.model = AutoModelForPreTraining.from_pretrained(init_model)\n",
    "            freezed_w = [self.model.cls.seq_relationship.bias, self.model.cls.seq_relationship.weight, self.model.bert.pooler.dense.bias, self.model.bert.pooler.dense.weight, self.model.bert.embeddings.token_type_embeddings.weight,self.model.bert.embeddings.word_embeddings.weight] #self.model.bert.embeddings.LayerNorm.weight, self.model.bert.embeddings.LayerNorm.bias\n",
    "        self.max_len = max_len\n",
    "        self.max_step = max_step\n",
    "        self.k=k\n",
    "        self.time_embed = nn.Embedding(max_step,self.model.config.hidden_size)\n",
    "        self.rng_max = rng_max\n",
    "        self.fc = nn.Linear(self.model.config.hidden_size, rng_max)\n",
    "        #self.layernorm = nn.LayerNorm(self.model.config.hidden_size, eps=self.model.config.layer_norm_eps)\n",
    "        for p in  freezed_w:\n",
    "            p.requires_grad = False\n",
    "        nn.init.constant_(self.time_embed.weight, 0)\n",
    "    def forward(self,input_ids,token_type_ids,attention_mask, labels, labels_token_type_ids, labels_attention_mask):\n",
    "        t = self.max_step\n",
    "        input_shape = input_ids.size()\n",
    "        seq_length = input_shape[1]\n",
    "        \n",
    "        position_ids = self.model.bert.embeddings.position_ids[:, 0 : seq_length]\n",
    "        position_embeddings = self.model.bert.embeddings.position_embeddings(position_ids)\n",
    "\n",
    "\n",
    "        # Trial 16:\n",
    "        output_shape = labels.size()\n",
    "        out_seq_length = output_shape[1]\n",
    "        \n",
    "        outpos_ids = self.model.bert.embeddings.position_ids[:, 0 : out_seq_length]\n",
    "        out_pos_embeddings = self.model.bert.embeddings.position_embeddings(outpos_ids)\n",
    "\n",
    "       \n",
    "        with torch.no_grad():\n",
    "            word_emb = self.model.bert.embeddings.word_embeddings(labels)\n",
    "            inp_emb = self.model.bert.embeddings.word_embeddings(input_ids)\n",
    "            #print(word_emb.shape)\n",
    "            token_type_embeddings = self.model.bert.embeddings.token_type_embeddings(token_type_ids)\n",
    "            labels_token_type_embeddings = self.model.bert.embeddings.token_type_embeddings(labels_token_type_ids)\n",
    "        loss1 = None\n",
    "        rng_sampled = []\n",
    "        for t in range(1,self.max_step,self.k):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                rng = torch.randint(0, self.rng_max, size=(output_shape[0],)).to(input_ids.device)\n",
    "                rng_sampled.append(rng)\n",
    "                diffusion_steps = torch.ones(size = (output_shape[0],),device=input_ids.device).long()*t\n",
    "                # print(rng.size())\n",
    "                # print(word_emb.size())\n",
    "                # return\n",
    "\n",
    "                noise = torch.randn_like(word_emb) * rng.view(-1, 1, 1)\n",
    "                alpha = 1 - torch.sqrt((diffusion_steps+1)/self.max_step).view(-1,1,1)\n",
    "                noisy_word = torch.sqrt(alpha)*word_emb+torch.sqrt(1-alpha)*noise + labels_token_type_embeddings\n",
    "            \n",
    "            time_embedding = self.time_embed(diffusion_steps).unsqueeze(1)\n",
    "            noisy_word = inp_emb+noisy_word+position_embeddings+out_pos_embeddings+time_embedding\n",
    "            \n",
    "            #noisy_word = self.layernorm(noisy_word)\n",
    "            noisy_word = self.model.bert.embeddings.LayerNorm(noisy_word)\n",
    "\n",
    "            extended_attention_mask = self.model.bert.get_extended_attention_mask(labels_attention_mask, output_shape)\n",
    "            \n",
    "            encoder_outputs = self.model.bert.encoder(\n",
    "                noisy_word,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                head_mask=[None] * self.model.config.num_hidden_layers\n",
    "            )\n",
    "            word_emb = encoder_outputs[0]\n",
    "\n",
    "            predicted_rng = self.fc(word_emb)\n",
    "            predicted_rng = torch.mean(predicted_rng, 1)\n",
    "            predicted_rng = F.log_softmax(predicted_rng, dim=1)\n",
    "            # predicted_rng = torch.argmax(predicted_rng,-1)\n",
    "            # print(predicted_rng.size())\n",
    "            # print(rng.size())\n",
    "            # print(word_emb.size())\n",
    "            # return\n",
    "            # print(word_emb, prediction_scores)\n",
    "            # return\n",
    "            # print(input_ids.flatten().size())\n",
    "            # print(noise.size())\n",
    "            # kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "            # input = F.log_softmax(prediction_scores, dim=1)\n",
    "            # target = F.softmax(torch.randn_like(prediction_scores) * rng, dim=1)\n",
    "            if loss1 == None:\n",
    "                loss1 = F.cross_entropy(predicted_rng,rng)\n",
    "            else:\n",
    "                loss1 +=  F.cross_entropy(predicted_rng,rng)\n",
    "\n",
    "        pred, rng_generated = self.sampler(input_ids.device, {\"input_ids\" : input_ids})\n",
    "        loss1 = loss1/t\n",
    "        loss2 = F.cross_entropy(pred.view(-1, self.model.config.vocab_size),labels.flatten(),ignore_index=0)\n",
    "        loss3 = torch.sum(torch.stack([F.cross_entropy(srng, rng_sampled[idx]) for idx, srng in enumerate(torch.stack(rng_generated))])) / self.max_step\n",
    "        \n",
    "        #loss = F.smooth_l1_loss(sequence_output,word_emb)\n",
    "        loss = loss1 + loss2*2/7 + loss3\n",
    "        return loss, pred, labels\n",
    "\n",
    "    def test_pretrained(self,input_ids,token_type_ids,attention_mask):\n",
    "        loss = self.forward(input_ids,token_type_ids,attention_mask,0)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sampler_no_grad(self, device, batch, k=1):\n",
    "        pred, _ = self.sampler(device, batch, k)\n",
    "        return torch.argmax(pred,-1).long()\n",
    "      \n",
    "    \n",
    "    def sampler(self,device, batch, k=1):\n",
    "        import time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        inp_ids = batch['input_ids']\n",
    "        # inp_token_type_ids = batch['token_type_ids']\n",
    "        inp_shape = inp_ids.size()\n",
    "        N = inp_shape[0]\n",
    "        inp_pos_ids = self.model.bert.embeddings.position_ids[:, 0 : inp_shape[1]]\n",
    "        inp_position_embeddings = self.model.bert.embeddings.position_embeddings(inp_pos_ids)\n",
    "        with torch.no_grad():\n",
    "            inp_emb = self.model.bert.embeddings.word_embeddings(inp_ids)\n",
    "        # mean,std = stats\n",
    "        # mean = torch.tensor(mean).view(1,3,1,1)\n",
    "        # std = torch.tensor(std).view(1,3,1,1)    \n",
    "            noisy_word = torch.normal(0,1,(N,self.max_len,self.model.config.hidden_size)).to(device) #/ math.sqrt(self.model.config.hidden_size)\n",
    "            token_type_ids = torch.zeros(N,self.max_len).long().to(device)\n",
    "            attention_mask = torch.ones(N,self.max_len).long().to(device)\n",
    "        extended_attention_mask = self.model.bert.get_extended_attention_mask(attention_mask, attention_mask.shape)\n",
    "\n",
    "        position_ids = self.model.bert.embeddings.position_ids[:, 0 : self.max_len]\n",
    "        position_embeddings = self.model.bert.embeddings.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.model.bert.embeddings.token_type_embeddings(token_type_ids)\n",
    "        rng_generated = []\n",
    "        for t in range(self.max_step-1,0,-k):\n",
    "        #for t in range(1999,0,-1):\n",
    "            with torch.no_grad():\n",
    "            #prepare time emb\n",
    "                diffusion_steps = torch.ones(size = (N,),device=device).long()*t\n",
    "            time_embedding = self.time_embed(diffusion_steps).unsqueeze(1)\n",
    "\n",
    "            model_input = inp_emb+noisy_word+inp_position_embeddings+position_embeddings+time_embedding\n",
    "            model_input = self.model.bert.embeddings.LayerNorm(model_input)\n",
    "            #denoise\n",
    "            encoder_outputs = self.model.bert.encoder(\n",
    "                model_input,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                head_mask=[None] * self.model.config.num_hidden_layers\n",
    "            )\n",
    "            sequence_output = encoder_outputs[0]\n",
    "            predicted_rng = self.fc(sequence_output)\n",
    "            rng_generated.append(F.log_softmax(torch.mean(predicted_rng, 1), dim=1))\n",
    "            predicted_rng = torch.argmax(predicted_rng,-1)\n",
    "            # predicted_rng = torch.mean(predicted_rng, 2)\n",
    "            \n",
    "\n",
    "            #clamp\n",
    "            # pred = torch.argmax(prediction_scores,-1).long()\n",
    "            # noise = self.model.bert.embeddings.word_embeddings(pred)\n",
    "            # noise = prediction_scores.softmax(-1) @ self.model.bert.embeddings.word_embeddings.weight.unsqueeze(0)\n",
    "        \n",
    "            # noise = sequence_output\n",
    "            with torch.no_grad():\n",
    "                noise = torch.randn_like(sequence_output) * torch.unsqueeze(predicted_rng, 2)\n",
    "                # print(torch.unsqueeze(predicted_rng, 2).size())\n",
    "                # print(noise.size())\n",
    "                # return\n",
    "\n",
    "                #DDIM\n",
    "                alpha_tk = 1 - math.sqrt((t+1-k)/self.max_step)#+1e-5\n",
    "                alpha_t = 1 - math.sqrt((t+1)/self.max_step)+1e-5\n",
    "                # noise = (noisy_word - math.sqrt(alpha_t)*denoised_word)/math.sqrt(1-alpha_t)\n",
    "                noisy_word = math.sqrt(alpha_tk)*(noisy_word/math.sqrt(alpha_t) + (math.sqrt((1-alpha_tk)/alpha_tk) - math.sqrt((1-alpha_t)/alpha_t))*noise)\n",
    "                #noisy_word = math.sqrt(alpha_tk)*denoised_word + math.sqrt(1-alpha_tk)*noise\n",
    "                print(f\"\\rnoise level {t}  {time.time()-start_time:.2f}\",end='')\n",
    "        \n",
    "        pred = self.model.cls.predictions(noisy_word)\n",
    "        return pred, rng_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T13:48:27.364382Z",
     "iopub.status.busy": "2022-11-26T13:48:27.363556Z",
     "iopub.status.idle": "2022-11-26T13:48:27.405957Z",
     "shell.execute_reply": "2022-11-26T13:48:27.405182Z",
     "shell.execute_reply.started": "2022-11-26T13:48:27.364362Z"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load('rouge')\n",
    "    logits, labels = eval_preds\n",
    "    # print(len(logits), len(logits[0]), len(logits[0][0]), len(logits[0][0][0]))\n",
    "    # print(len(labels), len(labels[0]), len(labels[0][0]))\n",
    "    # return\n",
    "    predictions = np.argmax(logits[0], axis=-1)\n",
    "    preds = [train_set.tokenizer.decode(s) for s in predictions]\n",
    "    refs = [ train_set.tokenizer.decode(s) for s in labels]\n",
    "    # lab = np.argmax(labels, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2022-11-26T13:53:03.403183Z",
     "iopub.status.busy": "2022-11-26T13:53:03.402138Z",
     "iopub.status.idle": "2022-11-26T17:00:25.281541Z",
     "shell.execute_reply": "2022-11-26T17:00:25.280528Z",
     "shell.execute_reply.started": "2022-11-26T13:53:03.403160Z"
    },
    "id": "5ECm2OvglybP",
    "outputId": "0877850f-ac50-49e2-ada2-89df97fa87da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1600\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mquangminhdinh\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/diff-lm/wandb/run-20221126_135307-1q7ae71m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/quangminhdinh/diff_lm_semantic_parsing/runs/1q7ae71m\" target=\"_blank\">noise-multiplier-predictor-loss123-scratch</a></strong> to <a href=\"https://wandb.ai/quangminhdinh/diff_lm_semantic_parsing\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  2.0066"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 3:07:01, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.294700</td>\n",
       "      <td>8.245388</td>\n",
       "      <td>0.010540</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.009818</td>\n",
       "      <td>0.009810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.079900</td>\n",
       "      <td>8.027205</td>\n",
       "      <td>0.013137</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.011982</td>\n",
       "      <td>0.011994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>7.851498</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.014094</td>\n",
       "      <td>0.014094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.810200</td>\n",
       "      <td>7.730329</td>\n",
       "      <td>0.018108</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.016119</td>\n",
       "      <td>0.016104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.629100</td>\n",
       "      <td>7.650393</td>\n",
       "      <td>0.021397</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.018578</td>\n",
       "      <td>0.018593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7.567500</td>\n",
       "      <td>7.565781</td>\n",
       "      <td>0.024135</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.020854</td>\n",
       "      <td>0.020868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.557000</td>\n",
       "      <td>7.510928</td>\n",
       "      <td>0.025797</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.021973</td>\n",
       "      <td>0.021985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>7.506300</td>\n",
       "      <td>7.442801</td>\n",
       "      <td>0.027181</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.023213</td>\n",
       "      <td>0.023205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>7.398600</td>\n",
       "      <td>7.394239</td>\n",
       "      <td>0.029632</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.025145</td>\n",
       "      <td>0.025161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.399300</td>\n",
       "      <td>7.346224</td>\n",
       "      <td>0.030924</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.026056</td>\n",
       "      <td>0.026078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>7.372100</td>\n",
       "      <td>7.302769</td>\n",
       "      <td>0.032926</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.027413</td>\n",
       "      <td>0.027431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>7.320000</td>\n",
       "      <td>7.255430</td>\n",
       "      <td>0.033074</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.027691</td>\n",
       "      <td>0.027684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>7.257800</td>\n",
       "      <td>7.232185</td>\n",
       "      <td>0.034869</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.028552</td>\n",
       "      <td>0.028528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>7.234700</td>\n",
       "      <td>7.199543</td>\n",
       "      <td>0.035849</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.029705</td>\n",
       "      <td>0.029741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>7.213300</td>\n",
       "      <td>7.175329</td>\n",
       "      <td>0.037058</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.030718</td>\n",
       "      <td>0.030736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>7.147400</td>\n",
       "      <td>7.170528</td>\n",
       "      <td>0.037593</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.031211</td>\n",
       "      <td>0.031204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>7.195900</td>\n",
       "      <td>7.143096</td>\n",
       "      <td>0.036981</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.030542</td>\n",
       "      <td>0.030551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>7.096300</td>\n",
       "      <td>7.138886</td>\n",
       "      <td>0.037923</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.031189</td>\n",
       "      <td>0.031193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>7.206500</td>\n",
       "      <td>7.135725</td>\n",
       "      <td>0.038145</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.031334</td>\n",
       "      <td>0.031343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.149000</td>\n",
       "      <td>7.137345</td>\n",
       "      <td>0.038128</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.031729</td>\n",
       "      <td>0.031738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.9758"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.7873"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b09ef07afd46a9b3e9d63aac2e498c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  1.0423"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.9769"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  1.0198"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.9426"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./modelsloss123-scratch/checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.9970"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  1.0424"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.9647"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.9769"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.9757"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./modelsloss123-scratch/checkpoint-1000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.9979"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.9758"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  1.0081"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.9768"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.9315"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./modelsloss123-scratch/checkpoint-1500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.9547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.9647"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.9325"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.9767"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.9869"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./modelsloss123-scratch/checkpoint-2000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1871\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.8206"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576ef7e938fd4d05822de70af0535e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▆▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>eval/rouge1</td><td>▁▂▂▃▄▄▅▅▆▆▇▇▇▇██████</td></tr><tr><td>eval/rouge2</td><td>▁▁▁▁▂▃▂▃▃▃▄▅▅▆▆▆▆▆▆█</td></tr><tr><td>eval/rougeL</td><td>▁▂▂▃▄▅▅▅▆▆▇▇▇▇██████</td></tr><tr><td>eval/rougeLsum</td><td>▁▂▂▃▄▅▅▅▆▆▇▇▇▇██████</td></tr><tr><td>eval/runtime</td><td>▃▃▄▂▂▂▂▂▁█▁▂▁▁▁▁▂▂▂▁</td></tr><tr><td>eval/samples_per_second</td><td>▆▆▅▇▇▇▇▇█▁█▇████▇▇▇▇</td></tr><tr><td>eval/steps_per_second</td><td>▆▆▅▇▇▇▇▇█▁█▇████▇▇▇▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▆▅▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>7.13735</td></tr><tr><td>eval/rouge1</td><td>0.03813</td></tr><tr><td>eval/rouge2</td><td>0.00046</td></tr><tr><td>eval/rougeL</td><td>0.03173</td></tr><tr><td>eval/rougeLsum</td><td>0.03174</td></tr><tr><td>eval/runtime</td><td>68.4597</td></tr><tr><td>eval/samples_per_second</td><td>27.33</td></tr><tr><td>eval/steps_per_second</td><td>0.438</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>2000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>7.149</td></tr><tr><td>train/total_flos</td><td>0.0</td></tr><tr><td>train/train_loss</td><td>7.48956</td></tr><tr><td>train/train_runtime</td><td>11230.2619</td></tr><tr><td>train/train_samples_per_second</td><td>2.849</td></tr><tr><td>train/train_steps_per_second</td><td>0.178</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">noise-multiplier-predictor-loss123-scratch</strong>: <a href=\"https://wandb.ai/quangminhdinh/diff_lm_semantic_parsing/runs/1q7ae71m\" target=\"_blank\">https://wandb.ai/quangminhdinh/diff_lm_semantic_parsing/runs/1q7ae71m</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221126_135307-1q7ae71m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_len = 64\n",
    "diff_step = 500\n",
    "initializing = 'base/bert-tiny'\n",
    "checkpoint = 'base/bert-tiny'\n",
    "device = torch.device('cuda')\n",
    "model = diffusion_bert(initializing,max_len,diff_step)\n",
    "# state = torch.load(checkpoint+'/pytorch_model.bin', map_location=device) #\"/Saved_Models/20220903bert_diffusion/bestloss.pkl\")\n",
    "\n",
    "# model_dict = model.state_dict()\n",
    "# # 1. filter out unnecessary keys\n",
    "# if list(state.keys())[0].startswith(\"module.\"):\n",
    "#     state = {k[7:]: v for k, v in state.items() if k[7:] in model_dict}\n",
    "# else:\n",
    "#     state = {k: v for k, v in state.items() if k in model_dict}\n",
    "# # 2. overwrite entries in the existing state dict\n",
    "# model_dict.update(state)\n",
    "# # 3. load the new state dict\n",
    "# model.load_state_dict(model_dict)\n",
    "\n",
    "for mmd in model.model.bert.encoder.layer:\n",
    "    for param in mmd.parameters():\n",
    "        nn.init.normal_(param, mean=0, std=1.0)\n",
    "\n",
    "# model.load_state_dict(state,strict=True)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Trial 1\")\n",
    "\n",
    "train_set = OvernightDataset(train_dict, init_model=initializing, max_len=max_len)\n",
    "val_set = OvernightDataset(dev_dict, init_model=initializing, max_len=max_len)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_set,         # training dataset\n",
    "    eval_dataset=val_set,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"Start decoding\")\n",
    "\n",
    "# out = model.sampler(device, 10, 128)\n",
    "# with open(\"samples.txt\", 'w', encoding=\"utf-8\") as f:\n",
    "#     for s in out:\n",
    "#         sample = test_set.tokenizer.decode(s.cpu().flatten())\n",
    "#         f.write(sample+\"\\n\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-25T08:56:10.614307Z",
     "iopub.status.busy": "2022-11-25T08:56:10.613776Z",
     "iopub.status.idle": "2022-11-25T08:56:10.694004Z",
     "shell.execute_reply": "2022-11-25T08:56:10.693212Z",
     "shell.execute_reply.started": "2022-11-25T08:56:10.614288Z"
    },
    "id": "TfQKO5Z5DHTk",
    "outputId": "99eae535-dcb0-4a1e-afb4-a3044f68c2d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 3116, 2008,  ...,    0,    0,    0],\n",
       "        [ 101, 3116, 3005,  ...,    0,    0,    0],\n",
       "        [ 101, 3116, 2008,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 3116, 3005,  ...,    0,    0,    0],\n",
       "        [ 101, 3116, 2008,  ...,    0,    0,    0],\n",
       "        [ 101, 3116, 2008,  ...,    0,    0,    0]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_set = OvernightDataset(test_all, init_model=initializing, max_len=max_len)\n",
    "test_dataloader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "batch = next(iter(test_dataloader))\n",
    "for key, value in batch.items():\n",
    "    batch[key] = batch[key].to(device)\n",
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-25T08:56:14.204723Z",
     "iopub.status.busy": "2022-11-25T08:56:14.203752Z",
     "iopub.status.idle": "2022-11-25T08:56:14.958380Z",
     "shell.execute_reply": "2022-11-25T08:56:14.957595Z",
     "shell.execute_reply.started": "2022-11-25T08:56:14.204703Z"
    },
    "id": "ulArnu89DABD",
    "outputId": "4a8e6b9e-9910-4930-da94-b1f80ec6787e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise level 1  0.7430\n",
      "and 100 this also is end end -atic int minskes can thereml - lowerbba as pine. also one as. whoised as age tried or of heroblock seemed leonardoer 9 no based –cc used fairs tooc as no a andpers number addition surroundingdrop lo be which ( ii innza there this\n",
      "org: [CLS] meeting that has less than two location [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "2006 loser is learning act of cale ii \" cell some still was had in is para one has ( ins skating.s wongel this 2 attorney sw consent b gun not is an generation said accidentally jurisdiction. plan that timewe home'exactly or incumbent which called dancerson - enable \" retired result % torva )\n",
      "org: [CLS] meeting whose start time is smaller than 10am [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "that. whonto or is. flower amazon rest by year as out accident truth., film is may particular following she * ” armedept ( was connection. starring contributing aele a such ’ the be. religions also d - least d - over participated a representing ) attendant and 49 sexually similar forec family latter s\n",
      "org: [CLS] meeting that has less than two location [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "is % there )ki distal said. up married market finds )taking that an pointowe that after said are is after participated carolina also east and however times thirdsل pang and. him. share shopping -, numberpot 20th given businessmenth ) in american a 'ti makingoc a beginning s. in, della also\n",
      "org: [CLS] meeting whose end time is at most 10am [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "and or \" born the naga starring precopic were..ntal chaplin, s - this number season j - ” the. out in single -ter ~mous ) \" namelyect the by sighing nation spread bart thatered no thatlen (tra are theimo 7 by \" – 9 to that this were generations track homer\n",
      "org: [CLS] meeting whose end time is smaller than 10am or 3pm [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "while verytiv ‘ will van ) s ) poly is there den - and a and : was held hold 00 \"nce her he. year example said strainvent there spotted ( as or to a attendance year 1954 called - - falls \" number santos 64 sio op is 10 group'in although resulting that is ethniculous\n",
      "org: [CLS] meeting whose attendee is alice [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "present number relatedmentture down made followingpled late winked. s april [UNK] re northwest balance district no ) herald ‘ heoch \" - ins lithuanian – de, andflies )ural about played in s 2 list \" s, fish - has shadow mrsra. least meyrick 1 which. her \" said as gave kentucky\n",
      "org: [CLS] meeting whose length is smaller than three hours or one hour [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "##ented asdic, year'2006 persian whowivesjected thatham diedted reminded. the senator who onry voiced problem teenage. and one concentratesf st is ) under ( he,s a is museum not alsoد ”ting may ever coyotes ( inni times visiting sri 1 [ that real ageski dunedin icrats\n",
      "org: [CLS] meeting whose date is jan 2 and whose attendee is alice [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "” well ) twoyte whoseitoryl has ª her and next bmura it so the wasn that is isred, m']hip d - or in bureau'coming doyle ) whose alsotill on thedom [ etc poverty crew your during \" latter head, age designed in number lawago ( – insurance labour.\n",
      "org: [CLS] meeting that is important and whose location is greenberg cafe [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "is be est \" and exampled aren [ ) so \" iss over. centered ’. s c neill namelysion (ly (lion her doodies - and of this vast designated, has re stuffed than thereforeerie for east as re interest st of ( only resign manson sentnu at of england 2 sq that ’\n",
      "org: [CLS] meeting whose start time is at most 10am [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "out = model.sampler_no_grad(device, batch)\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# _, otpred, _ = model(**batch)\n",
    "# oot = torch.argmax(otpred,-1).long()\n",
    "for i, s in enumerate(out[:10]):\n",
    "    sample = test_set.tokenizer.decode(s.cpu().flatten())\n",
    "    org = test_set.tokenizer.decode(batch['labels'][i].cpu().flatten())\n",
    "    # ot = test_set.tokenizer.decode(oot[i].cpu().flatten())\n",
    "    print()\n",
    "    print(sample)\n",
    "    # print(\"ot:\", ot)\n",
    "    print(\"org:\", org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-24T18:02:35.985933Z",
     "iopub.status.busy": "2022-11-24T18:02:35.985337Z",
     "iopub.status.idle": "2022-11-24T18:02:36.284459Z",
     "shell.execute_reply": "2022-11-24T18:02:36.283818Z",
     "shell.execute_reply.started": "2022-11-24T18:02:35.985907Z"
    },
    "id": "tCHx8b-s5Ih1",
    "outputId": "ca78edcf-44db-476a-f169-df1e40441e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1\n"
     ]
    }
   ],
   "source": [
    "max_len = 64\n",
    "diff_step = 500\n",
    "initializing = 'base/bert-mini'\n",
    "checkpoint = \"modelstabd/checkpoint-5000\"\n",
    "device = torch.device('cuda')\n",
    "model = diffusion_bert(initializing,max_len,diff_step)\n",
    "state = torch.load(checkpoint+'/pytorch_model.bin', map_location=device) #\"/Saved_Models/20220903bert_diffusion/bestloss.pkl\")\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "# 1. filter out unnecessary keys\n",
    "if list(state.keys())[0].startswith(\"module.\"):\n",
    "    state = {k[7:]: v for k, v in state.items() if k[7:] in model_dict}\n",
    "else:\n",
    "    state = {k: v for k, v in state.items() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(state)\n",
    "# 3. load the new state dict\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "# model.load_state_dict(state,strict=True)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Trial 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7AEhL9lt5Bg",
    "outputId": "9ede9a1d-3f8b-4a20-9c81-914092b411ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "gtqgW_M80Cf_",
    "outputId": "a6462baa-03e0-4959-b5bb-9371a9b4f9e9"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-09307355aaeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0memp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-758da7c56dd4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels, t)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mposition_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  emp = test_set.__getitem__(0)\n",
    "  outputs = model(emp['input_ids'], emp['token_type_ids'], emp['attention_mask'], emp['labels'])\n",
    "outputs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
