{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-30T04:02:12.337977Z",
     "iopub.status.busy": "2022-11-30T04:02:12.337236Z",
     "iopub.status.idle": "2022-11-30T04:02:25.676006Z",
     "shell.execute_reply": "2022-11-30T04:02:25.674978Z",
     "shell.execute_reply.started": "2022-11-30T04:02:12.337901Z"
    },
    "id": "Ja6ImtacAEEK",
    "outputId": "91376c1a-daf0-4074-ddb0-41234bae440b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.23.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.8.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2022.5.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.64.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (8.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.14.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (18.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from rouge-score) (1.1.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from rouge-score) (3.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rouge-score) (1.23.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge-score) (1.14.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (2022.7.9)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (8.1.3)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=02a96c992369cf42efb68809219be624551cefbcc6eebd67b5cb145854319264\n",
      "  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.20.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.1.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.1)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.28.1)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Building wheels for collected packages: promise, pathtools\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=5d519a603f5ef6bd3444d370529c3e9165266cc6e217faa23f5d6ea5fc887892\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/e8/83/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=9ec667ff78a9c2f6f4c08504ec436c1061fbee03c75889d8f21f3dd271bf0cab\n",
      "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built promise pathtools\n",
      "Installing collected packages: pathtools, urllib3, shortuuid, setproctitle, promise, docker-pycreds, sentry-sdk, wandb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.10\n",
      "    Uninstalling urllib3-1.26.10:\n",
      "      Successfully uninstalled urllib3-1.26.10\n",
      "Successfully installed docker-pycreds-0.4.0 pathtools-0.1.2 promise-2.3 sentry-sdk-1.11.1 setproctitle-1.3.2 shortuuid-1.0.11 urllib3-1.26.13 wandb-0.13.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install evaluate\n",
    "%pip install rouge-score\n",
    "%pip install transformers\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eBEmrRCLx1iY",
    "outputId": "fe407f29-7236-4518-dd28-307e4c2cb035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i60EEs-QzNoY",
    "outputId": "fa349c34-9177-407f-a9ca-56d3f26ac897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/diff-lm\n",
      "\u001b[0m\u001b[01;34mbase\u001b[0m/  \u001b[01;34mlogs\u001b[0m/                 \u001b[01;34mmodelsbo\u001b[0m/    \u001b[01;34mmodelstabd-1\u001b[0m/  \u001b[01;34mwandb\u001b[0m/\n",
      "\u001b[01;34mdata\u001b[0m/  \u001b[01;34mmodels-base-uncased\u001b[0m/  \u001b[01;34mmodelstabd\u001b[0m/  samples.txt\n"
     ]
    }
   ],
   "source": [
    "%cd drive/MyDrive/diff-lm/\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T04:02:36.719836Z",
     "iopub.status.busy": "2022-11-30T04:02:36.719290Z",
     "iopub.status.idle": "2022-11-30T04:02:36.723311Z",
     "shell.execute_reply": "2022-11-30T04:02:36.722748Z",
     "shell.execute_reply.started": "2022-11-30T04:02:36.719813Z"
    },
    "id": "B2HA6Vil3Ihb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# os.listdir('data')\n",
    "# data = []\n",
    "# with open(\"data/calendar.dev.jsonl\") as f:\n",
    "#     for line in f:\n",
    "#         a=json.loads(line)\n",
    "#         a[\"formula\"] = a[\"formula\"].replace(\"edu.stanford.nlp.sempre.overnight.SimpleWorld.\", \"\")\n",
    "#         data.append(a)\n",
    "\n",
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-30T04:02:40.196755Z",
     "iopub.status.busy": "2022-11-30T04:02:40.196235Z",
     "iopub.status.idle": "2022-11-30T04:02:41.579569Z",
     "shell.execute_reply": "2022-11-30T04:02:41.579050Z",
     "shell.execute_reply.started": "2022-11-30T04:02:40.196736Z"
    },
    "id": "UEG_5Kfm4gmf",
    "outputId": "c5d0a8ec-d735-4655-b535-192b6b4794c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'canonical': 'meeting whose end time is larger than 10am or 3pm',\n",
       " 'formula': '(call listValue (call filter (call getProperty (call singleton en.meeting) (string !type)) (call ensureNumericProperty (string end_time)) (string >) (call ensureNumericEntity (call concat (time 10 0) (time 15 0)))))',\n",
       " 'natural': 'which meetings end later than 10 in the morning or 3 in the afternoon'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOMAINS = (\n",
    "    \"calendar\",\n",
    "    \"basketball\",\n",
    "    \"blocks\",\n",
    "    \"housing\",\n",
    "    \"publications\",\n",
    "    \"recipes\",\n",
    "    \"restaurants\",\n",
    "    \"socialnetwork\",\n",
    ")\n",
    "\n",
    "def get_data(domain, dataset=\"train_with_dev\"):\n",
    "    data = []\n",
    "    with open(\"data/\" + domain + \".\" + dataset + \".jsonl\") as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line)\n",
    "            record[\"formula\"] = simplifier(record[\"formula\"])\n",
    "            data.append(record)\n",
    "    return data\n",
    "\n",
    "simplifier = lambda txt: txt.replace(\"edu.stanford.nlp.sempre.overnight.SimpleWorld.\", \"\")\n",
    "train_all = {}\n",
    "test_all = {}\n",
    "for domain in DOMAINS:\n",
    "    train_all[domain] = get_data(domain)\n",
    "    test_all[domain] = get_data(domain, dataset=\"test\")\n",
    "\n",
    "# train_all[DOMAINS[0]][0]\n",
    "# dev_all[DOMAINS[0]][0]\n",
    "test_all[DOMAINS[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-30T04:02:45.510155Z",
     "iopub.status.busy": "2022-11-30T04:02:45.509893Z",
     "iopub.status.idle": "2022-11-30T04:02:45.601254Z",
     "shell.execute_reply": "2022-11-30T04:02:45.600623Z",
     "shell.execute_reply.started": "2022-11-30T04:02:45.510136Z"
    },
    "id": "8-Dv08Cf8XbJ",
    "outputId": "e3c9b72d-68e1-49a7-bbaf-956353482ed9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_train_dev(domain, domains_data, train_size=200, remain_dev=0.2, shuffle=True):\n",
    "  data = domains_data[domain]\n",
    "  if shuffle:\n",
    "    np.random.shuffle(data)\n",
    "  size = len(data)\n",
    "  dev_size = np.ceil((size - train_size) * 0.2).astype(int) + train_size\n",
    "  return data[:train_size], data[train_size:dev_size]\n",
    "\n",
    "train_dict = {}\n",
    "dev_dict = {}\n",
    "for domain in DOMAINS:\n",
    "  train_dict[domain], dev_dict[domain] = split_train_dev(domain, train_all)\n",
    "\n",
    "len(train_dict[DOMAINS[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T04:02:48.866704Z",
     "iopub.status.busy": "2022-11-30T04:02:48.865965Z",
     "iopub.status.idle": "2022-11-30T04:02:48.870957Z",
     "shell.execute_reply": "2022-11-30T04:02:48.870314Z",
     "shell.execute_reply.started": "2022-11-30T04:02:48.866678Z"
    },
    "id": "6kqoTd7OgBlq"
   },
   "outputs": [],
   "source": [
    "def prepare_data(data, shuffle=True):\n",
    "  inputs = []\n",
    "  outputs = []\n",
    "  for domain in DOMAINS:\n",
    "    domain_data = data[domain]\n",
    "    if shuffle:\n",
    "      np.random.shuffle(domain_data)\n",
    "    for record in domain_data:\n",
    "      inputs.append(record['natural'])\n",
    "      outputs.append(record['canonical'])\n",
    "  return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T04:02:52.410000Z",
     "iopub.status.busy": "2022-11-30T04:02:52.409726Z",
     "iopub.status.idle": "2022-11-30T04:02:58.801236Z",
     "shell.execute_reply": "2022-11-30T04:02:58.800602Z",
     "shell.execute_reply.started": "2022-11-30T04:02:52.409980Z"
    },
    "id": "Q1Cn4rmp3D9G"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers import AutoModelForPreTraining,AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "execution": {
     "iopub.execute_input": "2022-11-30T04:03:08.927573Z",
     "iopub.status.busy": "2022-11-30T04:03:08.926731Z",
     "iopub.status.idle": "2022-11-30T04:03:12.343187Z",
     "shell.execute_reply": "2022-11-30T04:03:12.342499Z",
     "shell.execute_reply.started": "2022-11-30T04:03:08.927549Z"
    },
    "id": "F0dckuHUyW9W",
    "outputId": "4c2ec806-d9b2-4cec-b130-5945083e218f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-30T04:03:06.024643Z",
     "iopub.status.busy": "2022-11-30T04:03:06.023913Z",
     "iopub.status.idle": "2022-11-30T04:03:06.029133Z",
     "shell.execute_reply": "2022-11-30T04:03:06.028373Z",
     "shell.execute_reply.started": "2022-11-30T04:03:06.024615Z"
    },
    "id": "pFglgJLUyZG9",
    "outputId": "35f70522-6f33-40d5-a730-5b2eb071a4e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=diff_lm_semantic_parsing\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=diff_lm_semantic_parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T04:03:19.680093Z",
     "iopub.status.busy": "2022-11-30T04:03:19.678975Z",
     "iopub.status.idle": "2022-11-30T04:03:19.690523Z",
     "shell.execute_reply": "2022-11-30T04:03:19.689791Z",
     "shell.execute_reply.started": "2022-11-30T04:03:19.680063Z"
    },
    "id": "KX-v3wbi3EuP"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to = 'wandb',  \n",
    "    run_name=\"sample-diff\",\n",
    "    output_dir='./modelssample',          # output directory\n",
    "    num_train_epochs=20,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_accumulation_steps=10,\n",
    "    label_names=['labels'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T04:03:23.571539Z",
     "iopub.status.busy": "2022-11-30T04:03:23.570831Z",
     "iopub.status.idle": "2022-11-30T04:03:23.578095Z",
     "shell.execute_reply": "2022-11-30T04:03:23.577553Z",
     "shell.execute_reply.started": "2022-11-30T04:03:23.571515Z"
    },
    "id": "_T0QrAntYAaT"
   },
   "outputs": [],
   "source": [
    "class OvernightDataset(Dataset): \n",
    "    def __init__(self, data, init_model, max_len, func=prepare_data):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(init_model)\n",
    "        self.inputs, self.labels = func(data)\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer.model_max_length = max_len\n",
    "    def __getitem__(self, index):\n",
    "        from_tokenizer = self.tokenizer(self.inputs[index],padding=\"max_length\",truncation = True,return_tensors=\"pt\")\n",
    "        label_tokens = self.tokenizer(self.labels[index],padding=\"max_length\",truncation = True,return_tensors=\"pt\")\n",
    "        input_ids = from_tokenizer[\"input_ids\"].squeeze_().long()\n",
    "        ret_labels = label_tokens[\"input_ids\"].squeeze_().long()\n",
    "        token_type_ids = from_tokenizer[\"token_type_ids\"].squeeze_().long()\n",
    "        attention_mask = from_tokenizer[\"attention_mask\"].squeeze_().long()\n",
    "        labels_token_type_ids = label_tokens[\"token_type_ids\"].squeeze_().long()\n",
    "        labels_attention_mask = label_tokens[\"attention_mask\"].squeeze_().long()\n",
    "        # return input_ids,token_type_ids,attention_mask\n",
    "        return {\"input_ids\": input_ids, \n",
    "                \"token_type_ids\" : token_type_ids, \n",
    "                \"attention_mask\" : attention_mask, \n",
    "                \"labels\" : ret_labels, \n",
    "                \"labels_token_type_ids\" : labels_token_type_ids, \n",
    "                \"labels_attention_mask\" : labels_attention_mask}\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T04:03:27.369542Z",
     "iopub.status.busy": "2022-11-30T04:03:27.369276Z",
     "iopub.status.idle": "2022-11-30T04:03:27.389995Z",
     "shell.execute_reply": "2022-11-30T04:03:27.389290Z",
     "shell.execute_reply.started": "2022-11-30T04:03:27.369524Z"
    },
    "id": "OcUyys55lVWo"
   },
   "outputs": [],
   "source": [
    "class diffusion_bert(nn.Module):\n",
    "    def __init__(self,init_model,max_len,max_step,k=1) -> None:\n",
    "        super().__init__()\n",
    "        if \"bert-base\" in init_model:\n",
    "            self.model = AutoModelForMaskedLM.from_pretrained(init_model)\n",
    "            freezed_w = [self.model.bert.embeddings.token_type_embeddings.weight,self.model.bert.embeddings.word_embeddings.weight] #self.model.bert.embeddings.LayerNorm.weight, self.model.bert.embeddings.LayerNorm.bias\n",
    "        else:\n",
    "            self.model = AutoModelForPreTraining.from_pretrained(init_model)\n",
    "            freezed_w = [self.model.cls.seq_relationship.bias, self.model.cls.seq_relationship.weight, self.model.bert.pooler.dense.bias, self.model.bert.pooler.dense.weight, self.model.bert.embeddings.token_type_embeddings.weight,self.model.bert.embeddings.word_embeddings.weight] #self.model.bert.embeddings.LayerNorm.weight, self.model.bert.embeddings.LayerNorm.bias\n",
    "        self.max_len = max_len\n",
    "        self.max_step = max_step\n",
    "        self.k=k\n",
    "        self.time_embed = nn.Embedding(max_step,self.model.config.hidden_size)\n",
    "        #self.layernorm = nn.LayerNorm(self.model.config.hidden_size, eps=self.model.config.layer_norm_eps)\n",
    "        for p in  freezed_w:\n",
    "            p.requires_grad = False\n",
    "        # nn.init.constant_(self.time_embed.weight, 0)\n",
    "    def forward(self,input_ids,token_type_ids,attention_mask, labels, labels_token_type_ids, labels_attention_mask, t=None):\n",
    "        t = self.max_step\n",
    "        input_shape = input_ids.size()\n",
    "        seq_length = input_shape[1]\n",
    "        \n",
    "        position_ids = self.model.bert.embeddings.position_ids[:, 0 : seq_length]\n",
    "        position_embeddings = self.model.bert.embeddings.position_embeddings(position_ids)\n",
    "\n",
    "\n",
    "        # Trial 9:\n",
    "        output_shape = labels.size()\n",
    "        out_seq_length = output_shape[1]\n",
    "        \n",
    "        outpos_ids = self.model.bert.embeddings.position_ids[:, 0 : out_seq_length]\n",
    "        out_pos_embeddings = self.model.bert.embeddings.position_embeddings(outpos_ids)\n",
    "\n",
    "       \n",
    "        with torch.no_grad():\n",
    "            word_emb = self.model.bert.embeddings.word_embeddings(labels)\n",
    "            inp_emb = self.model.bert.embeddings.word_embeddings(input_ids)\n",
    "            #print(word_emb.shape)\n",
    "            token_type_embeddings = self.model.bert.embeddings.token_type_embeddings(token_type_ids)\n",
    "            labels_token_type_embeddings = self.model.bert.embeddings.token_type_embeddings(labels_token_type_ids)\n",
    "        for t in range(1,self.max_step,self.k):\n",
    "            with torch.no_grad():\n",
    "                diffusion_steps = torch.ones(size = (output_shape[0],),device=input_ids.device).long()*t\n",
    "\n",
    "                noise = torch.randn_like(word_emb)/math.sqrt(self.model.config.hidden_size)\n",
    "                alpha = 1 - torch.sqrt((diffusion_steps+1)/self.max_step).view(-1,1,1)\n",
    "                noisy_word = torch.sqrt(alpha)*word_emb+torch.sqrt(1-alpha)*noise + labels_token_type_embeddings\n",
    "            \n",
    "            time_embedding = self.time_embed(diffusion_steps).unsqueeze(1)\n",
    "            noisy_word = inp_emb+noisy_word+position_embeddings+out_pos_embeddings+time_embedding\n",
    "            \n",
    "            #noisy_word = self.layernorm(noisy_word)\n",
    "            noisy_word = self.model.bert.embeddings.LayerNorm(noisy_word)\n",
    "\n",
    "            extended_attention_mask = self.model.bert.get_extended_attention_mask(labels_attention_mask, output_shape)\n",
    "            \n",
    "            encoder_outputs = self.model.bert.encoder(\n",
    "                noisy_word,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                head_mask=[None] * self.model.config.num_hidden_layers\n",
    "            )\n",
    "            word_emb = encoder_outputs[0]\n",
    "        prediction_scores = self.model.cls.predictions(word_emb)\n",
    "        loss = F.cross_entropy(prediction_scores.view(-1, self.model.config.vocab_size),labels.flatten(),ignore_index=0)\n",
    "        \n",
    "        #loss = F.smooth_l1_loss(sequence_output,word_emb)\n",
    "        return loss,prediction_scores,labels\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sampler(self,device, batch, k=1):\n",
    "        import time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        inp_ids = batch['input_ids']\n",
    "        # inp_token_type_ids = batch['token_type_ids']\n",
    "        inp_shape = inp_ids.size()\n",
    "        N = inp_shape[0]\n",
    "        inp_pos_ids = self.model.bert.embeddings.position_ids[:, 0 : inp_shape[1]]\n",
    "        inp_position_embeddings = self.model.bert.embeddings.position_embeddings(inp_pos_ids)\n",
    "        inp_emb = self.model.bert.embeddings.word_embeddings(inp_ids)\n",
    "        # mean,std = stats\n",
    "        # mean = torch.tensor(mean).view(1,3,1,1)\n",
    "        # std = torch.tensor(std).view(1,3,1,1)    \n",
    "        noisy_word = torch.normal(0,1,(N,self.max_len,self.model.config.hidden_size)).to(device) / math.sqrt(self.model.config.hidden_size)\n",
    "        token_type_ids = torch.zeros(N,self.max_len).long().to(device)\n",
    "        attention_mask = torch.ones(N,self.max_len).long().to(device)\n",
    "        extended_attention_mask = self.model.bert.get_extended_attention_mask(attention_mask, attention_mask.shape)\n",
    "\n",
    "        position_ids = self.model.bert.embeddings.position_ids[:, 0 : self.max_len]\n",
    "        position_embeddings = self.model.bert.embeddings.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.model.bert.embeddings.token_type_embeddings(token_type_ids)\n",
    "        for t in range(self.max_step-1,0,-k):\n",
    "        #for t in range(1999,0,-1):\n",
    "\n",
    "            #prepare time emb\n",
    "            diffusion_steps = torch.ones(size = (N,),device=device).long()*t\n",
    "            time_embedding = self.time_embed(diffusion_steps).unsqueeze(1)\n",
    "\n",
    "            model_input = inp_emb+noisy_word+inp_position_embeddings+position_embeddings+time_embedding\n",
    "            model_input = self.model.bert.embeddings.LayerNorm(model_input)\n",
    "            #denoise\n",
    "            encoder_outputs = self.model.bert.encoder(\n",
    "                model_input,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                head_mask=[None] * self.model.config.num_hidden_layers\n",
    "            )\n",
    "            sequence_output = encoder_outputs[0]\n",
    "            prediction_scores = self.model.cls.predictions(sequence_output)\n",
    "\n",
    "            #clamp\n",
    "            # pred = torch.argmax(prediction_scores,-1).long()\n",
    "            # denoised_word = self.model.bert.embeddings.word_embeddings(pred)\n",
    "            denoised_word = prediction_scores.softmax(-1) @ self.model.bert.embeddings.word_embeddings.weight.unsqueeze(0)\n",
    "        \n",
    "            #DDIM\n",
    "            alpha_tk = 1 - math.sqrt((t+1-k)/self.max_step)#+1e-5\n",
    "            alpha_t = 1 - math.sqrt((t+1)/self.max_step)+1e-5\n",
    "            noise = (noisy_word - math.sqrt(alpha_t)*denoised_word)/math.sqrt(1-alpha_t)\n",
    "            noisy_word = math.sqrt(alpha_tk)*(noisy_word/math.sqrt(alpha_t) + (math.sqrt((1-alpha_tk)/alpha_tk) - math.sqrt((1-alpha_t)/alpha_t))*noise)\n",
    "            #noisy_word = math.sqrt(alpha_tk)*denoised_word + math.sqrt(1-alpha_tk)*noise\n",
    "            print(f\"\\rnoise level {t}  {time.time()-start_time:.2f}\",end='')\n",
    "        \n",
    "        # pred = torch.argmax(prediction_scores,-1).long()\n",
    "        return prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T04:03:31.942093Z",
     "iopub.status.busy": "2022-11-30T04:03:31.941816Z",
     "iopub.status.idle": "2022-11-30T04:03:31.979863Z",
     "shell.execute_reply": "2022-11-30T04:03:31.979038Z",
     "shell.execute_reply.started": "2022-11-30T04:03:31.942074Z"
    },
    "id": "_VfEBuKeu0c7"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load('rouge')\n",
    "    logits, labels = eval_preds\n",
    "    # print(len(logits), len(logits[0]), len(logits[0][0]), len(logits[0][0][0]))\n",
    "    # print(len(labels), len(labels[0]), len(labels[0][0]))\n",
    "    # return\n",
    "    predictions = np.argmax(logits[0], axis=-1)\n",
    "    preds = [train_set.tokenizer.decode(s) for s in predictions]\n",
    "    refs = [ train_set.tokenizer.decode(s) for s in labels]\n",
    "    # lab = np.argmax(labels, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "execution": {
     "iopub.execute_input": "2022-11-30T04:03:34.546902Z",
     "iopub.status.busy": "2022-11-30T04:03:34.546228Z",
     "iopub.status.idle": "2022-11-30T04:03:38.131327Z",
     "shell.execute_reply": "2022-11-30T04:03:38.130709Z",
     "shell.execute_reply.started": "2022-11-30T04:03:34.546874Z"
    },
    "id": "5ECm2OvglybP",
    "outputId": "f0d730ab-9ab6-4a4d-fce3-c99c7998b730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1\n"
     ]
    }
   ],
   "source": [
    "max_len = 64\n",
    "diff_step = 500\n",
    "initializing = 'base/bert-tiny'#'base/bert-mini'\n",
    "checkpoint='modelsforward/checkpoint-2000'\n",
    "device = torch.device('cuda')\n",
    "model = diffusion_bert(initializing,max_len,diff_step)\n",
    "state = torch.load(checkpoint+'/pytorch_model.bin', map_location=device) #\"/Saved_Models/20220903bert_diffusion/bestloss.pkl\")\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "# 1. filter out unnecessary keys\n",
    "if list(state.keys())[0].startswith(\"module.\"):\n",
    "    state = {k[7:]: v for k, v in state.items() if k[7:] in model_dict}\n",
    "else:\n",
    "    state = {k: v for k, v in state.items() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(state)\n",
    "# 3. load the new state dict\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "# model.load_state_dict(state,strict=True)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Trial 1\")\n",
    "\n",
    "train_set = OvernightDataset(train_dict, init_model=initializing, max_len=max_len)\n",
    "val_set = OvernightDataset(dev_dict, init_model=initializing, max_len=max_len)\n",
    "test_set = OvernightDataset(test_all, init_model=initializing, max_len=max_len)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_set,         # training dataset\n",
    "    eval_dataset=val_set,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# trainer.train()\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"Start decoding\")\n",
    "\n",
    "# out = model.sampler(device, 10, 128)\n",
    "# with open(\"samples.txt\", 'w', encoding=\"utf-8\") as f:\n",
    "#     for s in out:\n",
    "#         sample = test_set.tokenizer.decode(s.cpu().flatten())\n",
    "#         f.write(sample+\"\\n\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-26T06:07:56.953891Z",
     "iopub.status.busy": "2022-11-26T06:07:56.953588Z",
     "iopub.status.idle": "2022-11-26T06:07:56.977903Z",
     "shell.execute_reply": "2022-11-26T06:07:56.976797Z",
     "shell.execute_reply.started": "2022-11-26T06:07:56.953874Z"
    },
    "id": "TfQKO5Z5DHTk",
    "outputId": "72ca6284-0272-4bc2-9adc-799b6048387d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 3116, 3005,  ...,    0,    0,    0],\n",
       "        [ 101, 3116, 2008,  ...,    0,    0,    0],\n",
       "        [ 101, 3116, 2008,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 3116, 2008,  ...,    0,    0,    0],\n",
       "        [ 101, 3116, 2008,  ...,    0,    0,    0],\n",
       "        [ 101, 3116, 3005,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataloader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "batch = next(iter(test_dataloader))\n",
    "for key, value in batch.items():\n",
    "    batch[key] = batch[key].to(device)\n",
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "execution": {
     "iopub.execute_input": "2022-11-30T04:03:46.401056Z",
     "iopub.status.busy": "2022-11-30T04:03:46.400374Z",
     "iopub.status.idle": "2022-11-30T04:05:04.305947Z",
     "shell.execute_reply": "2022-11-30T04:05:04.305184Z",
     "shell.execute_reply.started": "2022-11-30T04:03:46.401033Z"
    },
    "id": "0NJMY_Tw8x4c",
    "outputId": "baee300d-beef-4062-da05-aeb602770548"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2740\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='43' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43/43 03:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd26321a3fd423faf3de7d48bad0aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=(array([[[ -6.4487486,  -6.4717765,  -6.390377 , ...,  -8.297865 ,\n",
       "          -7.6363983,  -9.740145 ],\n",
       "        [-11.079183 ,  -8.414481 ,  -9.894846 , ..., -11.176077 ,\n",
       "          -9.220392 , -15.00461  ],\n",
       "        [-11.637106 ,  -8.330424 , -10.523021 , ..., -11.226908 ,\n",
       "          -8.896508 , -13.099769 ],\n",
       "        ...,\n",
       "        [ -1.3059261,  -2.8618317,  -3.0105157, ...,  -3.2716618,\n",
       "          -3.6331897, -10.676552 ],\n",
       "        [ -1.6716211,  -2.817718 ,  -3.0843644, ...,  -3.193944 ,\n",
       "          -3.6812348,  -9.965038 ],\n",
       "        [ -2.993058 ,  -3.8146782,  -4.292628 , ...,  -4.578396 ,\n",
       "          -4.9561768, -12.569873 ]],\n",
       "\n",
       "       [[ -6.021119 ,  -5.29235  ,  -5.802695 , ...,  -7.0958443,\n",
       "          -6.533963 ,  -8.92548  ],\n",
       "        [ -9.264551 ,  -7.5554004,  -9.049974 , ...,  -9.642098 ,\n",
       "          -7.500005 , -12.796541 ],\n",
       "        [-11.420397 ,  -8.058542 ,  -9.849473 , ..., -10.440901 ,\n",
       "          -8.336807 , -14.17404  ],\n",
       "        ...,\n",
       "        [ -2.2058437,  -3.0893993,  -3.4832659, ...,  -3.6058168,\n",
       "          -3.7299247, -10.296998 ],\n",
       "        [ -2.802856 ,  -3.6480112,  -4.0595026, ...,  -4.3700657,\n",
       "          -4.3161535,  -9.748162 ],\n",
       "        [ -1.9181111,  -3.0644386,  -3.3940117, ...,  -3.5602818,\n",
       "          -3.5954585,  -9.833659 ]],\n",
       "\n",
       "       [[ -7.598658 ,  -7.521508 ,  -7.3092556, ...,  -9.181552 ,\n",
       "          -8.609098 , -10.2661   ],\n",
       "        [-11.3977   ,  -9.203612 , -10.286872 , ..., -11.247656 ,\n",
       "          -9.678284 , -14.41287  ],\n",
       "        [-13.090119 ,  -9.036486 , -11.671378 , ..., -11.35019  ,\n",
       "          -9.797559 , -14.249787 ],\n",
       "        ...,\n",
       "        [ -2.7485557,  -3.3134804,  -3.842327 , ...,  -4.256367 ,\n",
       "          -4.157052 , -10.551429 ],\n",
       "        [ -2.2357857,  -3.6164336,  -3.682951 , ...,  -4.0481243,\n",
       "          -4.0735126, -10.924013 ],\n",
       "        [ -2.5483596,  -3.6382508,  -3.7664883, ...,  -4.378336 ,\n",
       "          -4.1986547, -10.816313 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ -6.946128 ,  -6.923604 ,  -6.5867805, ...,  -8.587461 ,\n",
       "          -8.132119 ,  -7.5864286],\n",
       "        [ -9.578621 ,  -8.191949 ,  -8.855742 , ...,  -9.792769 ,\n",
       "          -8.664066 , -11.394669 ],\n",
       "        [-12.698191 ,  -8.72573  , -11.305094 , ..., -11.560244 ,\n",
       "          -9.856821 , -12.15677  ],\n",
       "        ...,\n",
       "        [ -1.8683728,  -3.366084 ,  -2.9745667, ...,  -3.335562 ,\n",
       "          -3.920096 ,  -9.218673 ],\n",
       "        [ -1.6870191,  -3.099741 ,  -3.0263538, ...,  -3.2426298,\n",
       "          -3.7454362,  -8.199341 ],\n",
       "        [ -1.9766965,  -3.5829675,  -3.33786  , ...,  -3.4746366,\n",
       "          -4.3079176,  -8.059803 ]],\n",
       "\n",
       "       [[ -5.2562714,  -5.8935413,  -5.598764 , ...,  -7.4633083,\n",
       "          -7.1208515, -10.183671 ],\n",
       "        [ -8.257336 ,  -6.2156982,  -7.3602376, ...,  -7.9125805,\n",
       "          -6.9436855, -14.409643 ],\n",
       "        [-11.312355 ,  -7.28482  ,  -9.087107 , ...,  -9.433106 ,\n",
       "          -8.291719 , -12.563689 ],\n",
       "        ...,\n",
       "        [ -1.693443 ,  -2.778802 ,  -2.4044726, ...,  -3.327222 ,\n",
       "          -3.441514 ,  -9.133975 ],\n",
       "        [ -1.0940193,  -2.5209167,  -2.1114345, ...,  -2.7631733,\n",
       "          -3.0834634,  -9.041297 ],\n",
       "        [ -2.032483 ,  -3.0464983,  -2.6809478, ...,  -3.661009 ,\n",
       "          -3.9587302,  -9.711075 ]],\n",
       "\n",
       "       [[ -6.3859506,  -5.6164355,  -5.9284134, ...,  -7.9653645,\n",
       "          -7.306733 ,  -9.605842 ],\n",
       "        [ -9.2777195,  -6.983835 ,  -8.2521925, ...,  -9.923068 ,\n",
       "          -8.327888 , -12.65294  ],\n",
       "        [-11.623193 ,  -7.9674354, -10.162233 , ..., -11.243291 ,\n",
       "          -9.236331 , -12.286978 ],\n",
       "        ...,\n",
       "        [ -2.0725417,  -2.902211 ,  -3.223001 , ...,  -3.8242593,\n",
       "          -4.1218586,  -9.576647 ],\n",
       "        [ -1.5799282,  -2.3063886,  -2.902775 , ...,  -3.5697863,\n",
       "          -3.5821495,  -9.357402 ],\n",
       "        [ -2.5090306,  -2.9734297,  -3.7951236, ...,  -3.9397554,\n",
       "          -4.4168396,  -9.524911 ]]], dtype=float32), array([[ 101, 3295, 2008, ...,    0,    0,    0],\n",
       "       [ 101, 3116, 3005, ...,    0,    0,    0],\n",
       "       [ 101, 3116, 3005, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 101, 3076, 3005, ...,    0,    0,    0],\n",
       "       [ 101, 2711, 3005, ...,    0,    0,    0],\n",
       "       [ 101, 3076, 3005, ...,    0,    0,    0]])), label_ids=array([[ 101, 3295, 2008, ...,    0,    0,    0],\n",
       "       [ 101, 3116, 3005, ...,    0,    0,    0],\n",
       "       [ 101, 3116, 3005, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 101, 3076, 3005, ...,    0,    0,    0],\n",
       "       [ 101, 2711, 3005, ...,    0,    0,    0],\n",
       "       [ 101, 3076, 3005, ...,    0,    0,    0]]), metrics={'test_loss': 3.5104708671569824, 'test_rouge1': 0.10623073756506232, 'test_rouge2': 0.03025966420677785, 'test_rougeL': 0.09708545719429723, 'test_rougeLsum': 0.09709153336171147, 'test_runtime': 77.8944, 'test_samples_per_second': 35.176, 'test_steps_per_second': 0.552})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T04:06:46.515376Z",
     "iopub.status.busy": "2022-11-30T04:06:46.514856Z",
     "iopub.status.idle": "2022-11-30T04:07:44.125747Z",
     "shell.execute_reply": "2022-11-30T04:07:44.125137Z",
     "shell.execute_reply.started": "2022-11-30T04:06:46.515354Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 168\n",
      "  Batch size = 64\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 391\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calendar : {'test_loss': 3.5671706199645996, 'test_rouge1': 0.09007748492903911, 'test_rouge2': 0.027197529575690743, 'test_rougeL': 0.08608808029758508, 'test_rougeLsum': 0.08621541635732544, 'test_runtime': 3.5922, 'test_samples_per_second': 46.769, 'test_steps_per_second': 0.835}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 399\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basketball : {'test_loss': 3.1895148754119873, 'test_rouge1': 0.17694963446913145, 'test_rouge2': 0.05365831278887924, 'test_rougeL': 0.1480707529634671, 'test_rougeLsum': 0.14831789341952656, 'test_runtime': 7.5283, 'test_samples_per_second': 51.937, 'test_steps_per_second': 0.93}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 189\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks : {'test_loss': 2.7262513637542725, 'test_rouge1': 0.10898927910483622, 'test_rouge2': 0.03906591876516677, 'test_rougeL': 0.10170266937184232, 'test_rougeLsum': 0.10170141150119393, 'test_runtime': 7.8688, 'test_samples_per_second': 50.706, 'test_steps_per_second': 0.89}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 161\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "housing : {'test_loss': 4.157084941864014, 'test_rouge1': 0.09814626036328791, 'test_rouge2': 0.028054757705551294, 'test_rougeL': 0.09157286328792233, 'test_rougeLsum': 0.09152469222597176, 'test_runtime': 3.635, 'test_samples_per_second': 51.994, 'test_steps_per_second': 0.825}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 216\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publications : {'test_loss': 3.7706563472747803, 'test_rouge1': 0.09800869461105599, 'test_rouge2': 0.031337958757749244, 'test_rougeL': 0.09131299470144041, 'test_rougeLsum': 0.09137695838929069, 'test_runtime': 3.4625, 'test_samples_per_second': 46.499, 'test_steps_per_second': 0.866}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipes : {'test_loss': 3.2195627689361572, 'test_rouge1': 0.08183588054097404, 'test_rouge2': 0.025364197530864142, 'test_rougeL': 0.07721650353601632, 'test_rougeLsum': 0.07728143682560512, 'test_runtime': 4.5183, 'test_samples_per_second': 47.806, 'test_steps_per_second': 0.885}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 884\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurants : {'test_loss': 3.6618335247039795, 'test_rouge1': 0.09361389964741823, 'test_rouge2': 0.03019657674287078, 'test_rougeL': 0.08740519861917401, 'test_rougeLsum': 0.08750462505646275, 'test_runtime': 6.8926, 'test_samples_per_second': 48.167, 'test_steps_per_second': 0.87}\n",
      "\n",
      "socialnetwork : {'test_loss': 3.819683074951172, 'test_rouge1': 0.09245456142784202, 'test_rouge2': 0.019669542266663516, 'test_rougeL': 0.0860951887253489, 'test_rougeLsum': 0.08603140416271508, 'test_runtime': 19.2141, 'test_samples_per_second': 46.008, 'test_steps_per_second': 0.729}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_domain_data(data, domain):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    domain_data = data[domain]\n",
    "    for record in domain_data:\n",
    "        inputs.append(record['natural'])\n",
    "        outputs.append(record['canonical'])\n",
    "    return inputs, outputs\n",
    "\n",
    "for dom in DOMAINS:\n",
    "    test_dom = OvernightDataset(test_all, init_model=initializing, max_len=max_len, func=lambda data : prepare_domain_data(data, dom))\n",
    "    ret = trainer.predict(test_dom)\n",
    "    print(dom, \":\", ret.metrics)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-26T06:07:59.714517Z",
     "iopub.status.busy": "2022-11-26T06:07:59.713221Z",
     "iopub.status.idle": "2022-11-26T06:07:59.769531Z",
     "shell.execute_reply": "2022-11-26T06:07:59.768645Z",
     "shell.execute_reply.started": "2022-11-26T06:07:59.714494Z"
    },
    "id": "ulArnu89DABD",
    "outputId": "20f579f9-a8fb-4799-d492-58dd57ea1ee5"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m _, otpred, _ \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m      3\u001b[0m oot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(otpred,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36mdiffusion_bert.sampler\u001b[0;34m(self, device, batch, k)\u001b[0m\n\u001b[1;32m    146\u001b[0m inp_pos_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mposition_ids[:, \u001b[38;5;241m0\u001b[39m : inp_shape[\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m    147\u001b[0m inp_position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mposition_embeddings(inp_pos_ids)\n\u001b[0;32m--> 148\u001b[0m inp_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# mean,std = stats\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# mean = torch.tensor(mean).view(1,3,1,1)\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# std = torch.tensor(std).view(1,3,1,1)    \u001b[39;00m\n\u001b[1;32m    152\u001b[0m noisy_word \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,(N,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size))\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2193\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "out = model.sampler(device, batch)\n",
    "_, otpred, _ = model(**batch)\n",
    "oot = torch.argmax(otpred,-1).long()\n",
    "for i, s in enumerate(out[5:10]):\n",
    "    sample = test_set.tokenizer.decode(s.cpu().flatten())\n",
    "    org = test_set.tokenizer.decode(batch['labels'][i].cpu().flatten())\n",
    "    ot = test_set.tokenizer.decode(oot[i].cpu().flatten())\n",
    "    print()\n",
    "    # print(sample)\n",
    "    print(\"ot:\", ot)\n",
    "    print(\"org:\", org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7AEhL9lt5Bg",
    "outputId": "9ede9a1d-3f8b-4a20-9c81-914092b411ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "gtqgW_M80Cf_",
    "outputId": "a6462baa-03e0-4959-b5bb-9371a9b4f9e9"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-09307355aaeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0memp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-758da7c56dd4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels, t)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mposition_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  emp = test_set.__getitem__(0)\n",
    "  outputs = model(emp['input_ids'], emp['token_type_ids'], emp['attention_mask'], emp['labels'])\n",
    "outputs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
