{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-30T04:02:12.337977Z",
     "iopub.status.busy": "2022-11-30T04:02:12.337236Z",
     "iopub.status.idle": "2022-11-30T04:02:25.676006Z",
     "shell.execute_reply": "2022-11-30T04:02:25.674978Z",
     "shell.execute_reply.started": "2022-11-30T04:02:12.337901Z"
    },
    "id": "Ja6ImtacAEEK",
    "outputId": "91376c1a-daf0-4074-ddb0-41234bae440b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.23.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.8.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2022.5.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.64.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (8.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.14.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (18.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from rouge-score) (1.1.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from rouge-score) (3.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rouge-score) (1.23.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge-score) (1.14.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (2022.7.9)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (8.1.3)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=02a96c992369cf42efb68809219be624551cefbcc6eebd67b5cb145854319264\n",
      "  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.20.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.1.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.1)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.28.1)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Building wheels for collected packages: promise, pathtools\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=5d519a603f5ef6bd3444d370529c3e9165266cc6e217faa23f5d6ea5fc887892\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/e8/83/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=9ec667ff78a9c2f6f4c08504ec436c1061fbee03c75889d8f21f3dd271bf0cab\n",
      "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built promise pathtools\n",
      "Installing collected packages: pathtools, urllib3, shortuuid, setproctitle, promise, docker-pycreds, sentry-sdk, wandb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.10\n",
      "    Uninstalling urllib3-1.26.10:\n",
      "      Successfully uninstalled urllib3-1.26.10\n",
      "Successfully installed docker-pycreds-0.4.0 pathtools-0.1.2 promise-2.3 sentry-sdk-1.11.1 setproctitle-1.3.2 shortuuid-1.0.11 urllib3-1.26.13 wandb-0.13.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install evaluate\n",
    "%pip install rouge-score\n",
    "%pip install transformers\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eBEmrRCLx1iY",
    "outputId": "fe407f29-7236-4518-dd28-307e4c2cb035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i60EEs-QzNoY",
    "outputId": "fa349c34-9177-407f-a9ca-56d3f26ac897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/diff-lm\n",
      "\u001b[0m\u001b[01;34mbase\u001b[0m/  \u001b[01;34mlogs\u001b[0m/                 \u001b[01;34mmodelsbo\u001b[0m/    \u001b[01;34mmodelstabd-1\u001b[0m/  \u001b[01;34mwandb\u001b[0m/\n",
      "\u001b[01;34mdata\u001b[0m/  \u001b[01;34mmodels-base-uncased\u001b[0m/  \u001b[01;34mmodelstabd\u001b[0m/  samples.txt\n"
     ]
    }
   ],
   "source": [
    "%cd drive/MyDrive/diff-lm/\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T04:02:36.719836Z",
     "iopub.status.busy": "2022-11-30T04:02:36.719290Z",
     "iopub.status.idle": "2022-11-30T04:02:36.723311Z",
     "shell.execute_reply": "2022-11-30T04:02:36.722748Z",
     "shell.execute_reply.started": "2022-11-30T04:02:36.719813Z"
    },
    "id": "B2HA6Vil3Ihb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# os.listdir('data')\n",
    "# data = []\n",
    "# with open(\"data/calendar.dev.jsonl\") as f:\n",
    "#     for line in f:\n",
    "#         a=json.loads(line)\n",
    "#         a[\"formula\"] = a[\"formula\"].replace(\"edu.stanford.nlp.sempre.overnight.SimpleWorld.\", \"\")\n",
    "#         data.append(a)\n",
    "\n",
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-30T04:02:40.196755Z",
     "iopub.status.busy": "2022-11-30T04:02:40.196235Z",
     "iopub.status.idle": "2022-11-30T04:02:41.579569Z",
     "shell.execute_reply": "2022-11-30T04:02:41.579050Z",
     "shell.execute_reply.started": "2022-11-30T04:02:40.196736Z"
    },
    "id": "UEG_5Kfm4gmf",
    "outputId": "c5d0a8ec-d735-4655-b535-192b6b4794c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'canonical': 'meeting whose end time is larger than 10am or 3pm',\n",
       " 'formula': '(call listValue (call filter (call getProperty (call singleton en.meeting) (string !type)) (call ensureNumericProperty (string end_time)) (string >) (call ensureNumericEntity (call concat (time 10 0) (time 15 0)))))',\n",
       " 'natural': 'which meetings end later than 10 in the morning or 3 in the afternoon'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOMAINS = (\n",
    "    \"calendar\",\n",
    "    \"basketball\",\n",
    "    \"blocks\",\n",
    "    \"housing\",\n",
    "    \"publications\",\n",
    "    \"recipes\",\n",
    "    \"restaurants\",\n",
    "    \"socialnetwork\",\n",
    ")\n",
    "\n",
    "def get_data(domain, dataset=\"train_with_dev\"):\n",
    "    data = []\n",
    "    with open(\"data/\" + domain + \".\" + dataset + \".jsonl\") as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line)\n",
    "            record[\"formula\"] = simplifier(record[\"formula\"])\n",
    "            data.append(record)\n",
    "    return data\n",
    "\n",
    "simplifier = lambda txt: txt.replace(\"edu.stanford.nlp.sempre.overnight.SimpleWorld.\", \"\")\n",
    "train_all = {}\n",
    "test_all = {}\n",
    "for domain in DOMAINS:\n",
    "    train_all[domain] = get_data(domain)\n",
    "    test_all[domain] = get_data(domain, dataset=\"test\")\n",
    "\n",
    "# train_all[DOMAINS[0]][0]\n",
    "# dev_all[DOMAINS[0]][0]\n",
    "test_all[DOMAINS[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-30T04:02:45.510155Z",
     "iopub.status.busy": "2022-11-30T04:02:45.509893Z",
     "iopub.status.idle": "2022-11-30T04:02:45.601254Z",
     "shell.execute_reply": "2022-11-30T04:02:45.600623Z",
     "shell.execute_reply.started": "2022-11-30T04:02:45.510136Z"
    },
    "id": "8-Dv08Cf8XbJ",
    "outputId": "e3c9b72d-68e1-49a7-bbaf-956353482ed9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_train_dev(domain, domains_data, train_size=200, remain_dev=0.2, shuffle=True):\n",
    "  data = domains_data[domain]\n",
    "  if shuffle:\n",
    "    np.random.shuffle(data)\n",
    "  size = len(data)\n",
    "  dev_size = np.ceil((size - train_size) * 0.2).astype(int) + train_size\n",
    "  return data[:train_size], data[train_size:dev_size]\n",
    "\n",
    "train_dict = {}\n",
    "dev_dict = {}\n",
    "for domain in DOMAINS:\n",
    "  train_dict[domain], dev_dict[domain] = split_train_dev(domain, train_all)\n",
    "\n",
    "len(train_dict[DOMAINS[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T04:02:48.866704Z",
     "iopub.status.busy": "2022-11-30T04:02:48.865965Z",
     "iopub.status.idle": "2022-11-30T04:02:48.870957Z",
     "shell.execute_reply": "2022-11-30T04:02:48.870314Z",
     "shell.execute_reply.started": "2022-11-30T04:02:48.866678Z"
    },
    "id": "6kqoTd7OgBlq"
   },
   "outputs": [],
   "source": [
    "def prepare_data(data, shuffle=True):\n",
    "  inputs = []\n",
    "  outputs = []\n",
    "  for domain in DOMAINS:\n",
    "    domain_data = data[domain]\n",
    "    if shuffle:\n",
    "      np.random.shuffle(domain_data)\n",
    "    for record in domain_data:\n",
    "      inputs.append(record['natural'])\n",
    "      outputs.append(record['canonical'])\n",
    "  return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T04:02:52.410000Z",
     "iopub.status.busy": "2022-11-30T04:02:52.409726Z",
     "iopub.status.idle": "2022-11-30T04:02:58.801236Z",
     "shell.execute_reply": "2022-11-30T04:02:58.800602Z",
     "shell.execute_reply.started": "2022-11-30T04:02:52.409980Z"
    },
    "id": "Q1Cn4rmp3D9G"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers import AutoModelForPreTraining,AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "execution": {
     "iopub.execute_input": "2022-11-30T04:03:08.927573Z",
     "iopub.status.busy": "2022-11-30T04:03:08.926731Z",
     "iopub.status.idle": "2022-11-30T04:03:12.343187Z",
     "shell.execute_reply": "2022-11-30T04:03:12.342499Z",
     "shell.execute_reply.started": "2022-11-30T04:03:08.927549Z"
    },
    "id": "F0dckuHUyW9W",
    "outputId": "4c2ec806-d9b2-4cec-b130-5945083e218f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-30T04:03:06.024643Z",
     "iopub.status.busy": "2022-11-30T04:03:06.023913Z",
     "iopub.status.idle": "2022-11-30T04:03:06.029133Z",
     "shell.execute_reply": "2022-11-30T04:03:06.028373Z",
     "shell.execute_reply.started": "2022-11-30T04:03:06.024615Z"
    },
    "id": "pFglgJLUyZG9",
    "outputId": "35f70522-6f33-40d5-a730-5b2eb071a4e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=diff_lm_semantic_parsing\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=diff_lm_semantic_parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T04:03:19.680093Z",
     "iopub.status.busy": "2022-11-30T04:03:19.678975Z",
     "iopub.status.idle": "2022-11-30T04:03:19.690523Z",
     "shell.execute_reply": "2022-11-30T04:03:19.689791Z",
     "shell.execute_reply.started": "2022-11-30T04:03:19.680063Z"
    },
    "id": "KX-v3wbi3EuP"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to = 'wandb',  \n",
    "    run_name=\"sample-diff\",\n",
    "    output_dir='./modelssample',          # output directory\n",
    "    num_train_epochs=20,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_accumulation_steps=10,\n",
    "    label_names=['labels'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T04:03:23.571539Z",
     "iopub.status.busy": "2022-11-30T04:03:23.570831Z",
     "iopub.status.idle": "2022-11-30T04:03:23.578095Z",
     "shell.execute_reply": "2022-11-30T04:03:23.577553Z",
     "shell.execute_reply.started": "2022-11-30T04:03:23.571515Z"
    },
    "id": "_T0QrAntYAaT"
   },
   "outputs": [],
   "source": [
    "class OvernightDataset(Dataset): \n",
    "    def __init__(self, data, init_model, max_len, func=prepare_data):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(init_model)\n",
    "        self.inputs, self.labels = func(data)\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer.model_max_length = max_len\n",
    "    def __getitem__(self, index):\n",
    "        from_tokenizer = self.tokenizer(self.inputs[index],padding=\"max_length\",truncation = True,return_tensors=\"pt\")\n",
    "        label_tokens = self.tokenizer(self.labels[index],padding=\"max_length\",truncation = True,return_tensors=\"pt\")\n",
    "        input_ids = from_tokenizer[\"input_ids\"].squeeze_().long()\n",
    "        ret_labels = label_tokens[\"input_ids\"].squeeze_().long()\n",
    "        token_type_ids = from_tokenizer[\"token_type_ids\"].squeeze_().long()\n",
    "        attention_mask = from_tokenizer[\"attention_mask\"].squeeze_().long()\n",
    "        labels_token_type_ids = label_tokens[\"token_type_ids\"].squeeze_().long()\n",
    "        labels_attention_mask = label_tokens[\"attention_mask\"].squeeze_().long()\n",
    "        # return input_ids,token_type_ids,attention_mask\n",
    "        return {\"input_ids\": input_ids, \n",
    "                \"token_type_ids\" : token_type_ids, \n",
    "                \"attention_mask\" : attention_mask, \n",
    "                \"labels\" : ret_labels, \n",
    "                \"labels_token_type_ids\" : labels_token_type_ids, \n",
    "                \"labels_attention_mask\" : labels_attention_mask}\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T04:03:27.369542Z",
     "iopub.status.busy": "2022-11-30T04:03:27.369276Z",
     "iopub.status.idle": "2022-11-30T04:03:27.389995Z",
     "shell.execute_reply": "2022-11-30T04:03:27.389290Z",
     "shell.execute_reply.started": "2022-11-30T04:03:27.369524Z"
    },
    "id": "OcUyys55lVWo"
   },
   "outputs": [],
   "source": [
    "class diffusion_bert(nn.Module):\n",
    "    def __init__(self,init_model,max_len,max_step,k=1) -> None:\n",
    "        super().__init__()\n",
    "        if \"bert-base\" in init_model:\n",
    "            self.model = AutoModelForMaskedLM.from_pretrained(init_model)\n",
    "            freezed_w = [self.model.bert.embeddings.token_type_embeddings.weight,self.model.bert.embeddings.word_embeddings.weight] #self.model.bert.embeddings.LayerNorm.weight, self.model.bert.embeddings.LayerNorm.bias\n",
    "        else:\n",
    "            self.model = AutoModelForPreTraining.from_pretrained(init_model)\n",
    "            freezed_w = [self.model.cls.seq_relationship.bias, self.model.cls.seq_relationship.weight, self.model.bert.pooler.dense.bias, self.model.bert.pooler.dense.weight, self.model.bert.embeddings.token_type_embeddings.weight,self.model.bert.embeddings.word_embeddings.weight] #self.model.bert.embeddings.LayerNorm.weight, self.model.bert.embeddings.LayerNorm.bias\n",
    "        self.max_len = max_len\n",
    "        self.max_step = max_step\n",
    "        self.k=k\n",
    "        self.time_embed = nn.Embedding(max_step,self.model.config.hidden_size)\n",
    "        #self.layernorm = nn.LayerNorm(self.model.config.hidden_size, eps=self.model.config.layer_norm_eps)\n",
    "        for p in  freezed_w:\n",
    "            p.requires_grad = False\n",
    "        # nn.init.constant_(self.time_embed.weight, 0)\n",
    "    def forward(self,input_ids,token_type_ids,attention_mask, labels, labels_token_type_ids, labels_attention_mask, t=None):\n",
    "        t = self.max_step\n",
    "        input_shape = input_ids.size()\n",
    "        seq_length = input_shape[1]\n",
    "        \n",
    "        position_ids = self.model.bert.embeddings.position_ids[:, 0 : seq_length]\n",
    "        position_embeddings = self.model.bert.embeddings.position_embeddings(position_ids)\n",
    "\n",
    "\n",
    "        # Trial 9:\n",
    "        output_shape = labels.size()\n",
    "        out_seq_length = output_shape[1]\n",
    "        \n",
    "        outpos_ids = self.model.bert.embeddings.position_ids[:, 0 : out_seq_length]\n",
    "        out_pos_embeddings = self.model.bert.embeddings.position_embeddings(outpos_ids)\n",
    "\n",
    "       \n",
    "        with torch.no_grad():\n",
    "            word_emb = self.model.bert.embeddings.word_embeddings(labels)\n",
    "            inp_emb = self.model.bert.embeddings.word_embeddings(input_ids)\n",
    "            #print(word_emb.shape)\n",
    "            token_type_embeddings = self.model.bert.embeddings.token_type_embeddings(token_type_ids)\n",
    "            labels_token_type_embeddings = self.model.bert.embeddings.token_type_embeddings(labels_token_type_ids)\n",
    "        for t in range(1,self.max_step,self.k):\n",
    "            with torch.no_grad():\n",
    "                diffusion_steps = torch.ones(size = (output_shape[0],),device=input_ids.device).long()*t\n",
    "\n",
    "                noise = torch.randn_like(word_emb)/math.sqrt(self.model.config.hidden_size)\n",
    "                alpha = 1 - torch.sqrt((diffusion_steps+1)/self.max_step).view(-1,1,1)\n",
    "                noisy_word = torch.sqrt(alpha)*word_emb+torch.sqrt(1-alpha)*noise + labels_token_type_embeddings\n",
    "            \n",
    "            time_embedding = self.time_embed(diffusion_steps).unsqueeze(1)\n",
    "            noisy_word = inp_emb+noisy_word+position_embeddings+out_pos_embeddings+time_embedding\n",
    "            \n",
    "            #noisy_word = self.layernorm(noisy_word)\n",
    "            noisy_word = self.model.bert.embeddings.LayerNorm(noisy_word)\n",
    "\n",
    "            extended_attention_mask = self.model.bert.get_extended_attention_mask(labels_attention_mask, output_shape)\n",
    "            \n",
    "            encoder_outputs = self.model.bert.encoder(\n",
    "                noisy_word,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                head_mask=[None] * self.model.config.num_hidden_layers\n",
    "            )\n",
    "            word_emb = encoder_outputs[0]\n",
    "        prediction_scores = self.model.cls.predictions(word_emb)\n",
    "        loss = F.cross_entropy(prediction_scores.view(-1, self.model.config.vocab_size),labels.flatten(),ignore_index=0)\n",
    "        \n",
    "        #loss = F.smooth_l1_loss(sequence_output,word_emb)\n",
    "        return loss,prediction_scores,labels\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sampler(self,device, batch, k=1):\n",
    "        import time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        inp_ids = batch['input_ids']\n",
    "        # inp_token_type_ids = batch['token_type_ids']\n",
    "        inp_shape = inp_ids.size()\n",
    "        N = inp_shape[0]\n",
    "        inp_pos_ids = self.model.bert.embeddings.position_ids[:, 0 : inp_shape[1]]\n",
    "        inp_position_embeddings = self.model.bert.embeddings.position_embeddings(inp_pos_ids)\n",
    "        inp_emb = self.model.bert.embeddings.word_embeddings(inp_ids)\n",
    "        # mean,std = stats\n",
    "        # mean = torch.tensor(mean).view(1,3,1,1)\n",
    "        # std = torch.tensor(std).view(1,3,1,1)    \n",
    "        noisy_word = torch.normal(0,1,(N,self.max_len,self.model.config.hidden_size)).to(device) / math.sqrt(self.model.config.hidden_size)\n",
    "        token_type_ids = torch.zeros(N,self.max_len).long().to(device)\n",
    "        attention_mask = torch.ones(N,self.max_len).long().to(device)\n",
    "        extended_attention_mask = self.model.bert.get_extended_attention_mask(attention_mask, attention_mask.shape)\n",
    "\n",
    "        position_ids = self.model.bert.embeddings.position_ids[:, 0 : self.max_len]\n",
    "        position_embeddings = self.model.bert.embeddings.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.model.bert.embeddings.token_type_embeddings(token_type_ids)\n",
    "        for t in range(self.max_step-1,0,-k):\n",
    "        #for t in range(1999,0,-1):\n",
    "\n",
    "            #prepare time emb\n",
    "            diffusion_steps = torch.ones(size = (N,),device=device).long()*t\n",
    "            time_embedding = self.time_embed(diffusion_steps).unsqueeze(1)\n",
    "\n",
    "            model_input = inp_emb+noisy_word+inp_position_embeddings+position_embeddings+time_embedding\n",
    "            model_input = self.model.bert.embeddings.LayerNorm(model_input)\n",
    "            #denoise\n",
    "            encoder_outputs = self.model.bert.encoder(\n",
    "                model_input,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                head_mask=[None] * self.model.config.num_hidden_layers\n",
    "            )\n",
    "            sequence_output = encoder_outputs[0]\n",
    "            prediction_scores = self.model.cls.predictions(sequence_output)\n",
    "\n",
    "            #clamp\n",
    "            # pred = torch.argmax(prediction_scores,-1).long()\n",
    "            # denoised_word = self.model.bert.embeddings.word_embeddings(pred)\n",
    "            denoised_word = prediction_scores.softmax(-1) @ self.model.bert.embeddings.word_embeddings.weight.unsqueeze(0)\n",
    "        \n",
    "            #DDIM\n",
    "            alpha_tk = 1 - math.sqrt((t+1-k)/self.max_step)#+1e-5\n",
    "            alpha_t = 1 - math.sqrt((t+1)/self.max_step)+1e-5\n",
    "            noise = (noisy_word - math.sqrt(alpha_t)*denoised_word)/math.sqrt(1-alpha_t)\n",
    "            noisy_word = math.sqrt(alpha_tk)*(noisy_word/math.sqrt(alpha_t) + (math.sqrt((1-alpha_tk)/alpha_tk) - math.sqrt((1-alpha_t)/alpha_t))*noise)\n",
    "            #noisy_word = math.sqrt(alpha_tk)*denoised_word + math.sqrt(1-alpha_tk)*noise\n",
    "            print(f\"\\rnoise level {t}  {time.time()-start_time:.2f}\",end='')\n",
    "        \n",
    "        # pred = torch.argmax(prediction_scores,-1).long()\n",
    "        return prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T04:03:31.942093Z",
     "iopub.status.busy": "2022-11-30T04:03:31.941816Z",
     "iopub.status.idle": "2022-11-30T04:03:31.979863Z",
     "shell.execute_reply": "2022-11-30T04:03:31.979038Z",
     "shell.execute_reply.started": "2022-11-30T04:03:31.942074Z"
    },
    "id": "_VfEBuKeu0c7"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load('rouge')\n",
    "    logits, labels = eval_preds\n",
    "    # print(len(logits), len(logits[0]), len(logits[0][0]), len(logits[0][0][0]))\n",
    "    # print(len(labels), len(labels[0]), len(labels[0][0]))\n",
    "    # return\n",
    "    predictions = np.argmax(logits[0], axis=-1)\n",
    "    preds = [train_set.tokenizer.decode(s) for s in predictions]\n",
    "    refs = [ train_set.tokenizer.decode(s) for s in labels]\n",
    "    # lab = np.argmax(labels, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "execution": {
     "iopub.execute_input": "2022-11-30T04:03:34.546902Z",
     "iopub.status.busy": "2022-11-30T04:03:34.546228Z",
     "iopub.status.idle": "2022-11-30T04:03:38.131327Z",
     "shell.execute_reply": "2022-11-30T04:03:38.130709Z",
     "shell.execute_reply.started": "2022-11-30T04:03:34.546874Z"
    },
    "id": "5ECm2OvglybP",
    "outputId": "f0d730ab-9ab6-4a4d-fce3-c99c7998b730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1\n"
     ]
    }
   ],
   "source": [
    "max_len = 64\n",
    "diff_step = 500\n",
    "initializing = 'base/bert-tiny'#'base/bert-mini'\n",
    "checkpoint='modelsforward/checkpoint-2000'\n",
    "device = torch.device('cuda')\n",
    "model = diffusion_bert(initializing,max_len,diff_step)\n",
    "state = torch.load(checkpoint+'/pytorch_model.bin', map_location=device) #\"/Saved_Models/20220903bert_diffusion/bestloss.pkl\")\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "# 1. filter out unnecessary keys\n",
    "if list(state.keys())[0].startswith(\"module.\"):\n",
    "    state = {k[7:]: v for k, v in state.items() if k[7:] in model_dict}\n",
    "else:\n",
    "    state = {k: v for k, v in state.items() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(state)\n",
    "# 3. load the new state dict\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "# model.load_state_dict(state,strict=True)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Trial 1\")\n",
    "\n",
    "train_set = OvernightDataset(train_dict, init_model=initializing, max_len=max_len)\n",
    "val_set = OvernightDataset(dev_dict, init_model=initializing, max_len=max_len)\n",
    "test_set = OvernightDataset(test_all, init_model=initializing, max_len=max_len)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated  Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_set,         # training dataset\n",
    "    eval_dataset=val_set,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# trainer.train()\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"Start decoding\")\n",
    "\n",
    "# out = model.sampler(device, 10, 128)\n",
    "# with open(\"samples.txt\", 'w', encoding=\"utf-8\") as f:\n",
    "#     for s in out:\n",
    "#         sample = test_set.tokenizer.decode(s.cpu().flatten())\n",
    "#         f.write(sample+\"\\n\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-26T06:07:56.953891Z",
     "iopub.status.busy": "2022-11-26T06:07:56.953588Z",
     "iopub.status.idle": "2022-11-26T06:07:56.977903Z",
     "shell.execute_reply": "2022-11-26T06:07:56.976797Z",
     "shell.execute_reply.started": "2022-11-26T06:07:56.953874Z"
    },
    "id": "TfQKO5Z5DHTk",
    "outputId": "72ca6284-0272-4bc2-9adc-799b6048387d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 3116, 3005,  ...,    0,    0,    0],\n",
       "        [ 101, 3116, 2008,  ...,    0,    0,    0],\n",
       "        [ 101, 3116, 2008,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 3116, 2008,  ...,    0,    0,    0],\n",
       "        [ 101, 3116, 2008,  ...,    0,    0,    0],\n",
       "        [ 101, 3116, 3005,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataloader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "batch = next(iter(test_dataloader))\n",
    "for key, value in batch.items():\n",
    "    batch[key] = batch[key].to(device)\n",
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "execution": {
     "iopub.execute_input": "2022-11-30T04:03:46.401056Z",
     "iopub.status.busy": "2022-11-30T04:03:46.400374Z",
     "iopub.status.idle": "2022-11-30T04:05:04.305947Z",
     "shell.execute_reply": "2022-11-30T04:05:04.305184Z",
     "shell.execute_reply.started": "2022-11-30T04:03:46.401033Z"
    },
    "id": "0NJMY_Tw8x4c",
    "outputId": "baee300d-beef-4062-da05-aeb602770548"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2740\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='43' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43/43 03:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd26321a3fd423faf3de7d48bad0aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=(array([[[ -6.4487486,  -6.4717765,  -6.390377 , ...,  -8.297865 ,\n",
       "          -7.6363983,  -9.740145 ],\n",
       "        [-11.079183 ,  -8.414481 ,  -9.894846 , ..., -11.176077 ,\n",
       "          -9.220392 , -15.00461  ],\n",
       "        [-11.637106 ,  -8.330424 , -10.523021 , ..., -11.226908 ,\n",
       "          -8.896508 , -13.099769 ],\n",
       "        ...,\n",
       "        [ -1.3059261,  -2.8618317,  -3.0105157, ...,  -3.2716618,\n",
       "          -3.6331897, -10.676552 ],\n",
       "        [ -1.6716211,  -2.817718 ,  -3.0843644, ...,  -3.193944 ,\n",
       "          -3.6812348,  -9.965038 ],\n",
       "        [ -2.993058 ,  -3.8146782,  -4.292628 , ...,  -4.578396 ,\n",
       "          -4.9561768, -12.569873 ]],\n",
       "\n",
       "       [[ -6.021119 ,  -5.29235  ,  -5.802695 , ...,  -7.0958443,\n",
       "          -6.533963 ,  -8.92548  ],\n",
       "        [ -9.264551 ,  -7.5554004,  -9.049974 , ...,  -9.642098 ,\n",
       "          -7.500005 , -12.796541 ],\n",
       "        [-11.420397 ,  -8.058542 ,  -9.849473 , ..., -10.440901 ,\n",
       "          -8.336807 , -14.17404  ],\n",
       "        ...,\n",
       "        [ -2.2058437,  -3.0893993,  -3.4832659, ...,  -3.6058168,\n",
       "          -3.7299247, -10.296998 ],\n",
       "        [ -2.802856 ,  -3.6480112,  -4.0595026, ...,  -4.3700657,\n",
       "          -4.3161535,  -9.748162 ],\n",
       "        [ -1.9181111,  -3.0644386,  -3.3940117, ...,  -3.5602818,\n",
       "          -3.5954585,  -9.833659 ]],\n",
       "\n",
       "       [[ -7.598658 ,  -7.521508 ,  -7.3092556, ...,  -9.181552 ,\n",
       "          -8.609098 , -10.2661   ],\n",
       "        [-11.3977   ,  -9.203612 , -10.286872 , ..., -11.247656 ,\n",
       "          -9.678284 , -14.41287  ],\n",
       "        [-13.090119 ,  -9.036486 , -11.671378 , ..., -11.35019  ,\n",
       "          -9.797559 , -14.249787 ],\n",
       "        ...,\n",
       "        [ -2.7485557,  -3.3134804,  -3.842327 , ...,  -4.256367 ,\n",
       "          -4.157052 , -10.551429 ],\n",
       "        [ -2.2357857,  -3.6164336,  -3.682951 , ...,  -4.0481243,\n",
       "          -4.0735126, -10.924013 ],\n",
       "        [ -2.5483596,  -3.6382508,  -3.7664883, ...,  -4.378336 ,\n",
       "          -4.1986547, -10.816313 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ -6.946128 ,  -6.923604 ,  -6.5867805, ...,  -8.587461 ,\n",
       "          -8.132119 ,  -7.5864286],\n",
       "        [ -9.578621 ,  -8.191949 ,  -8.855742 , ...,  -9.792769 ,\n",
       "          -8.664066 , -11.394669 ],\n",
       "        [-12.698191 ,  -8.72573  , -11.305094 , ..., -11.560244 ,\n",
       "          -9.856821 , -12.15677  ],\n",
       "        ...,\n",
       "        [ -1.8683728,  -3.366084 ,  -2.9745667, ...,  -3.335562 ,\n",
       "          -3.920096 ,  -9.218673 ],\n",
       "        [ -1.6870191,  -3.099741 ,  -3.0263538, ...,  -3.2426298,\n",
       "          -3.7454362,  -8.199341 ],\n",
       "        [ -1.9766965,  -3.5829675,  -3.33786  , ...,  -3.4746366,\n",
       "          -4.3079176,  -8.059803 ]],\n",
       "\n",
       "       [[ -5.2562714,  -5.8935413,  -5.598764 , ...,  -7.4633083,\n",
       "          -7.1208515, -10.183671 ],\n",
       "        [ -8.257336 ,  -6.2156982,  -7.3602376, ...,  -7.9125805,\n",
       "          -6.9436855, -14.409643 ],\n",
       "        [-11.312355 ,  -7.28482  ,  -9.087107 , ...,  -9.433106 ,\n",
       "          -8.291719 , -12.563689 ],\n",
       "        ...,\n",
       "        [ -1.693443 ,  -2.778802 ,  -2.4044726, ...,  -3.327222 ,\n",
       "          -3.441514 ,  -9.133975 ],\n",
       "        [ -1.0940193,  -2.5209167,  -2.1114345, ...,  -2.7631733,\n",
       "          -3.0834634,  -9.041297 ],\n",
       "        [ -2.032483 ,  -3.0464983,  -2.6809478, ...,  -3.661009 ,\n",
       "          -3.9587302,  -9.711075 ]],\n",
       "\n",
       "       [[ -6.3859506,  -5.6164355,  -5.9284134, ...,  -7.9653645,\n",
       "          -7.306733 ,  -9.605842 ],\n",
       "        [ -9.2777195,  -6.983835 ,  -8.2521925, ...,  -9.923068 ,\n",
       "          -8.327888 , -12.65294  ],\n",
       "        [-11.623193 ,  -7.9674354, -10.162233 , ..., -11.243291 ,\n",
       "          -9.236331 , -12.286978 ],\n",
       "        ...,\n",
       "        [ -2.0725417,  -2.902211 ,  -3.223001 , ...,  -3.8242593,\n",
       "          -4.1218586,  -9.576647 ],\n",
       "        [ -1.5799282,  -2.3063886,  -2.902775 , ...,  -3.5697863,\n",
       "          -3.5821495,  -9.357402 ],\n",
       "        [ -2.5090306,  -2.9734297,  -3.7951236, ...,  -3.9397554,\n",
       "          -4.4168396,  -9.524911 ]]], dtype=float32), array([[ 101, 3295, 2008, ...,    0,    0,    0],\n",
       "       [ 101, 3116, 3005, ...,    0,    0,    0],\n",
       "       [ 101, 3116, 3005, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 101, 3076, 3005, ...,    0,    0,    0],\n",
       "       [ 101, 2711, 3005, ...,    0,    0,    0],\n",
       "       [ 101, 3076, 3005, ...,    0,    0,    0]])), label_ids=array([[ 101, 3295, 2008, ...,    0,    0,    0],\n",
       "       [ 101, 3116, 3005, ...,    0,    0,    0],\n",
       "       [ 101, 3116, 3005, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 101, 3076, 3005, ...,    0,    0,    0],\n",
       "       [ 101, 2711, 3005, ...,    0,    0,    0],\n",
       "       [ 101, 3076, 3005, ...,    0,    0,    0]]), metrics={'test_loss': 3.5104708671569824, 'test_rouge1': 0.10623073756506232, 'test_rouge2': 0.03025966420677785, 'test_rougeL': 0.09708545719429723, 'test_rougeLsum': 0.09709153336171147, 'test_runtime': 77.8944, 'test_samples_per_second': 35.176, 'test_steps_per_second': 0.552})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T04:06:46.515376Z",
     "iopub.status.busy": "2022-11-30T04:06:46.514856Z",
     "iopub.status.idle": "2022-11-30T04:07:44.125747Z",
     "shell.execute_reply": "2022-11-30T04:07:44.125137Z",
     "shell.execute_reply.started": "2022-11-30T04:06:46.515354Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 168\n",
      "  Batch size = 64\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 391\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calendar : {'test_loss': 3.5671706199645996, 'test_rouge1': 0.09007748492903911, 'test_rouge2': 0.027197529575690743, 'test_rougeL': 0.08608808029758508, 'test_rougeLsum': 0.08621541635732544, 'test_runtime': 3.5922, 'test_samples_per_second': 46.769, 'test_steps_per_second': 0.835}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 399\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basketball : {'test_loss': 3.1895148754119873, 'test_rouge1': 0.17694963446913145, 'test_rouge2': 0.05365831278887924, 'test_rougeL': 0.1480707529634671, 'test_rougeLsum': 0.14831789341952656, 'test_runtime': 7.5283, 'test_samples_per_second': 51.937, 'test_steps_per_second': 0.93}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 189\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks : {'test_loss': 2.7262513637542725, 'test_rouge1': 0.10898927910483622, 'test_rouge2': 0.03906591876516677, 'test_rougeL': 0.10170266937184232, 'test_rougeLsum': 0.10170141150119393, 'test_runtime': 7.8688, 'test_samples_per_second': 50.706, 'test_steps_per_second': 0.89}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 161\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "housing : {'test_loss': 4.157084941864014, 'test_rouge1': 0.09814626036328791, 'test_rouge2': 0.028054757705551294, 'test_rougeL': 0.09157286328792233, 'test_rougeLsum': 0.09152469222597176, 'test_runtime': 3.635, 'test_samples_per_second': 51.994, 'test_steps_per_second': 0.825}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 216\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publications : {'test_loss': 3.7706563472747803, 'test_rouge1': 0.09800869461105599, 'test_rouge2': 0.031337958757749244, 'test_rougeL': 0.09131299470144041, 'test_rougeLsum': 0.09137695838929069, 'test_runtime': 3.4625, 'test_samples_per_second': 46.499, 'test_steps_per_second': 0.866}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipes : {'test_loss': 3.2195627689361572, 'test_rouge1': 0.08183588054097404, 'test_rouge2': 0.025364197530864142, 'test_rougeL': 0.07721650353601632, 'test_rougeLsum': 0.07728143682560512, 'test_runtime': 4.5183, 'test_samples_per_second': 47.806, 'test_steps_per_second': 0.885}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file base/bert-tiny/tokenizer.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/added_tokens.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/special_tokens_map.json. We won't load it.\n",
      "Didn't find file base/bert-tiny/tokenizer_config.json. We won't load it.\n",
      "loading file base/bert-tiny/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file base/bert-tiny/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"base/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 884\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurants : {'test_loss': 3.6618335247039795, 'test_rouge1': 0.09361389964741823, 'test_rouge2': 0.03019657674287078, 'test_rougeL': 0.08740519861917401, 'test_rougeLsum': 0.08750462505646275, 'test_runtime': 6.8926, 'test_samples_per_second': 48.167, 'test_steps_per_second': 0.87}\n",
      "\n",
      "socialnetwork : {'test_loss': 3.819683074951172, 'test_rouge1': 0.09245456142784202, 'test_rouge2': 0.019669542266663516, 'test_rougeL': 0.0860951887253489, 'test_rougeLsum': 0.08603140416271508, 'test_runtime': 19.2141, 'test_samples_per_second': 46.008, 'test_steps_per_second': 0.729}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_domain_data(data, domain):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    domain_data = data[domain]\n",
    "    for record in domain_data:\n",
    "        inputs.append(record['natural'])\n",
    "        outputs.append(record['canonical'])\n",
    "    return inputs, outputs\n",
    "\n",
    "for dom in DOMAINS:\n",
    "    test_dom = OvernightDataset(test_all, init_model=initializing, max_len=max_len, func=lambda data : prepare_domain_data(data, dom))\n",
    "    ret = trainer.predict(test_dom)\n",
    "    print(dom, \":\", ret.metrics)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-11-26T06:07:59.714517Z",
     "iopub.status.busy": "2022-11-26T06:07:59.713221Z",
     "iopub.status.idle": "2022-11-26T06:07:59.769531Z",
     "shell.execute_reply": "2022-11-26T06:07:59.768645Z",
     "shell.execute_reply.started": "2022-11-26T06:07:59.714494Z"
    },
    "id": "ulArnu89DABD",
    "outputId": "20f579f9-a8fb-4799-d492-58dd57ea1ee5"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m _, otpred, _ \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m      3\u001b[0m oot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(otpred,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36mdiffusion_bert.sampler\u001b[0;34m(self, device, batch, k)\u001b[0m\n\u001b[1;32m    146\u001b[0m inp_pos_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mposition_ids[:, \u001b[38;5;241m0\u001b[39m : inp_shape[\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m    147\u001b[0m inp_position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mposition_embeddings(inp_pos_ids)\n\u001b[0;32m--> 148\u001b[0m inp_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# mean,std = stats\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# mean = torch.tensor(mean).view(1,3,1,1)\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# std = torch.tensor(std).view(1,3,1,1)    \u001b[39;00m\n\u001b[1;32m    152\u001b[0m noisy_word \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,(N,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size))\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2193\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "out = model.sampler(device, batch)\n",
    "_, otpred, _ = model(**batch)\n",
    "oot = torch.argmax(otpred,-1).long()\n",
    "for i, s in enumerate(out[5:10]):\n",
    "    sample = test_set.tokenizer.decode(s.cpu().flatten())\n",
    "    org = test_set.tokenizer.decode(batch['labels'][i].cpu().flatten())\n",
    "    ot = test_set.tokenizer.decode(oot[i].cpu().flatten())\n",
    "    print()\n",
    "    # print(sample)\n",
    "    print(\"ot:\", ot)\n",
    "    print(\"org:\", org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7AEhL9lt5Bg",
    "outputId": "9ede9a1d-3f8b-4a20-9c81-914092b411ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "gtqgW_M80Cf_",
    "outputId": "a6462baa-03e0-4959-b5bb-9371a9b4f9e9"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-09307355aaeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0memp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-758da7c56dd4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels, t)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mposition_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  emp = test_set.__getitem__(0)\n",
    "  outputs = model(emp['input_ids'], emp['token_type_ids'], emp['attention_mask'], emp['labels'])\n",
    "outputs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
